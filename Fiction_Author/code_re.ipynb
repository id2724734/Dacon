{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import metrics, preprocessing, naive_bayes, model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling1D, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "X_train = pd.read_csv('open/train.csv')\n",
    "X_test = pd.read_csv( 'open/test_x.csv')\n",
    "\n",
    "# authors = [0,1,2,3,4]\n",
    "Y_train = LabelEncoder().fit_transform(X_train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소문자, 단어 나눔\n",
    "def clean(X_train,X_test):\n",
    "    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n",
    "    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n",
    "    return X_train,X_test\n",
    "X_train,X_test = clean(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì', 'ô', 'ù', '?', 'ü', 'ç', ';', '—', 'ï', 'î', ')', '*', '’', '”', '‘', '(', '“', 'Æ', 'ê', 'Œ', 'æ', 'ö', 'ñ', '/', '[', '‐', '-', 'ë', 'à', 'â', '!', 'ä', '\"', '}', \"'\", 'è', ':', '_', 'º', 'Ê', '£', ']', 'œ', '#', ',', '{', '&', 'é', '.']\n"
     ]
    }
   ],
   "source": [
    "# 특수문자 확인하기\n",
    "p = re.compile('[a-z]|[A-Z]|[0-9]')\n",
    "char = {}\n",
    "for text in X_train.text:\n",
    "    text = text.replace(' ', '')\n",
    "    for c in text:\n",
    "        char[c] = '_'\n",
    "char_list = list(char.keys())\n",
    "x = ' '.join(char_list)\n",
    "y = p.findall(x)\n",
    "sp_char_list = list(set(char_list) - set(y))\n",
    "print(sp_char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점 비율(문장 안에 각 부호가 얼마나 있는지)\n",
    "punctuations = [{\"id\":1, \"p\" : \"[;:]\"},\n",
    "                {\"id\":2, \"p\" : \"[,.]\"},\n",
    "                {\"id\":3, \"p\" : \"[?]\"},\n",
    "                {\"id\":4, \"p\" : \"[!]\"},\n",
    "                {\"id\":5, \"p\" : \"[‘’\\']\"},\n",
    "                {\"id\":6, \"p\" : \"[“”\\\"]\"},\n",
    "                {\"id\":7, \"p\" : \"[;:,.?!\\'“”‘’\\\"]\"}]\n",
    "\n",
    "for p in punctuations:\n",
    "    punctuation = p[\"p\"]\n",
    "    _train =  [sentence.split() for sentence in X_train['text']]\n",
    "    X_train['punc_' + str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _train]\n",
    "\n",
    "    _test =  [sentence.split() for sentence in X_test['text']]\n",
    "    X_test['punc_' + str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_val_y     :  [0.07813081 0.0178482  0.07129607 0.81539852 0.0173264 ]\n",
      "pred_test_y    :  [0.19020287 0.13951587 0.16309699 0.46061922 0.04656505]\n",
      "pred_full_test :  [0.19020287 0.13951587 0.16309699 0.46061922 0.04656505]\n",
      "pred_train     :  [0.07813081 0.0178482  0.07129607 0.81539852 0.0173264 ]\n",
      "\n",
      "pred_val_y     :  [0.22769558 0.08564251 0.31930272 0.21442673 0.15293246]\n",
      "pred_test_y    :  [0.18826039 0.15068576 0.15663496 0.45675144 0.04766744]\n",
      "pred_full_test :  [0.37846326 0.29020163 0.31973195 0.91737066 0.0942325 ]\n",
      "pred_train     :  [0.07813081 0.0178482  0.07129607 0.81539852 0.0173264 ]\n",
      "\n",
      "pred_val_y     :  [0.23746063 0.05438223 0.38674226 0.23436174 0.08705313]\n",
      "pred_test_y    :  [0.19612146 0.16123402 0.17670903 0.41781048 0.04812501]\n",
      "pred_full_test :  [0.57458472 0.45143565 0.49644098 1.33518114 0.14235751]\n",
      "pred_train     :  [0.07813081 0.0178482  0.07129607 0.81539852 0.0173264 ]\n",
      "\n",
      "pred_val_y     :  [0.15653621 0.05095289 0.37101161 0.19084719 0.23065209]\n",
      "pred_test_y    :  [0.2021808  0.14346695 0.16565542 0.44038309 0.04831373]\n",
      "pred_full_test :  [0.77676552 0.5949026  0.66209641 1.77556423 0.19067124]\n",
      "pred_train     :  [0.07813081 0.0178482  0.07129607 0.81539852 0.0173264 ]\n",
      "\n",
      "pred_val_y     :  [0.31712826 0.11400455 0.13562056 0.37396487 0.05928176]\n",
      "pred_test_y    :  [0.19485678 0.14862473 0.13600296 0.47030135 0.05021418]\n",
      "pred_full_test :  [0.9716223  0.74352733 0.79809937 2.24586558 0.24088542]\n",
      "pred_train     :  [0.07813081 0.0178482  0.07129607 0.81539852 0.0173264 ]\n",
      "\n",
      "Mean cv score :  1.1176035179779529\n"
     ]
    }
   ],
   "source": [
    "# tfidf - words - nb (학습의 단위를 단어로 설정)\n",
    "def tfidfWords(X_train, X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2), analyzer = 'word')\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf, test_tfidf, full_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_tfidf_MNB(X_train, X_test, Y_train):\n",
    "    train_tfidf, test_tfidf, full_tfidf = tfidfWords(X_train, X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 5])\n",
    "    kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        print('pred_val_y     : ', pred_val_y[0])\n",
    "        print('pred_test_y    : ', pred_test_y[0])\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        print('pred_full_test : ', pred_full_test[0])\n",
    "        pred_train[val_index, : ] = pred_val_y\n",
    "        print('pred_train     : ' , pred_train[0])\n",
    "        print('')\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train, pred_test = do_tfidf_MNB(X_train, X_test, Y_train)\n",
    "X_train[\"tfidf_words_nb_0\"] = pred_train[ : , 0]\n",
    "X_train[\"tfidf_words_nb_1\"] = pred_train[ : , 1]\n",
    "X_train[\"tfidf_words_nb_2\"] = pred_train[ : , 2]\n",
    "X_train[\"tfidf_words_nb_3\"] = pred_train[ : , 3]\n",
    "X_train[\"tfidf_words_nb_4\"] = pred_train[ : , 4]\n",
    "X_test[\"tfidf_words_nb_0\"] = pred_test[ : , 0]\n",
    "X_test[\"tfidf_words_nb_1\"] = pred_test[ : , 1]\n",
    "X_test[\"tfidf_words_nb_2\"] = pred_test[ : , 2]\n",
    "X_test[\"tfidf_words_nb_3\"] = pred_test[ : , 3]\n",
    "X_test[\"tfidf_words_nb_4\"] = pred_test[ : , 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  1.0896401622640728\n"
     ]
    }
   ],
   "source": [
    "# tfidf - chars - nb (학습의 단위를 글자로 설정)\n",
    "def tfidfWords(X_train, X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words = 'english', ngram_range = (1, 3), analyzer = 'char')\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf, test_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train, X_test, Y_train):\n",
    "    train_tfidf, test_tfidf = tfidfWords(X_train, X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 5])\n",
    "    kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index, : ] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5\n",
    "    return pred_train,pred_full_test\n",
    "pred_train,pred_test = do(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_chars_nb_0\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_chars_nb_1\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_chars_nb_2\"] = pred_train[:,2]\n",
    "X_train[\"tfidf_chars_nb_3\"] = pred_train[:,3]\n",
    "X_train[\"tfidf_chars_nb_4\"] = pred_train[:,4]\n",
    "X_test[\"tfidf_chars_nb_0\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_chars_nb_1\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_chars_nb_2\"] = pred_test[:,2]\n",
    "X_test[\"tfidf_chars_nb_3\"] = pred_test[:,3]\n",
    "X_test[\"tfidf_chars_nb_4\"] = pred_test[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  1.1484245877084394\n"
     ]
    }
   ],
   "source": [
    "# count - words - nb\n",
    "def countWords(X_train, X_test):\n",
    "    count_vec = CountVectorizer(stop_words = 'english', ngram_range=(1, 2), analyzer = 'word')\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count, test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_MNB(X_train, X_test, Y_train):\n",
    "    train_count, test_count=countWords(X_train, X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 5])\n",
    "    kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index, : ] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5\n",
    "    return pred_train, pred_full_test\n",
    "\n",
    "pred_train, pred_test = do_count_MNB(X_train, X_test, Y_train)\n",
    "X_train[\"count_words_nb_0\"] = pred_train[ : , 0]\n",
    "X_train[\"count_words_nb_1\"] = pred_train[ : , 1]\n",
    "X_train[\"count_words_nb_2\"] = pred_train[ : , 2]\n",
    "X_train[\"count_words_nb_3\"] = pred_train[ : , 3]\n",
    "X_train[\"count_words_nb_4\"] = pred_train[ : , 4]\n",
    "X_test[\"count_words_nb_0\"] = pred_test[ : , 0]\n",
    "X_test[\"count_words_nb_1\"] = pred_test[ : , 1]\n",
    "X_test[\"count_words_nb_2\"] = pred_test[ : , 2]\n",
    "X_test[\"count_words_nb_3\"] = pred_test[ : , 3]\n",
    "X_test[\"count_words_nb_4\"] = pred_test[ : , 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean cv score :  1.1792103306062116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  3.264365333331928\n"
     ]
    }
   ],
   "source": [
    "# count - chars - nb\n",
    "def countChars(X_train,X_test):\n",
    "    count_vec = CountVectorizer(ngram_range = (1, 3), analyzer = 'char')\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count, test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_chars_MNB(X_train, X_test, Y_train):\n",
    "    train_count, test_count = countChars(X_train, X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 5])\n",
    "    kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index, : ] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5\n",
    "    return pred_train, pred_full_test\n",
    "\n",
    "pred_train, pred_test = do_count_chars_MNB(X_train, X_test, Y_train)\n",
    "X_train[\"count_chars_nb_0\"] = pred_train[ : , 0]\n",
    "X_train[\"count_chars_nb_1\"] = pred_train[ : , 1]\n",
    "X_train[\"count_chars_nb_2\"] = pred_train[ : , 2]\n",
    "X_train[\"count_chars_nb_3\"] = pred_train[ : , 3]\n",
    "X_train[\"count_chars_nb_4\"] = pred_train[ : , 4]\n",
    "\n",
    "X_test[\"count_chars_nb_0\"] = pred_test[ : , 0]\n",
    "X_test[\"count_chars_nb_1\"] = pred_test[ : , 1]\n",
    "X_test[\"count_chars_nb_2\"] = pred_test[ : , 2]\n",
    "X_test[\"count_chars_nb_3\"] = pred_test[ : , 3]\n",
    "X_test[\"count_chars_nb_4\"] = pred_test[ : , 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean cv score :  5.806984089402464   --> (1, 7)\n",
    "# Mean cv score :  4.383101848616254   --> (1, 5)\n",
    "# Mean cv score :  3.264365333331928   --> (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.5541 - accuracy: 0.2915 - val_loss: 1.5215 - val_accuracy: 0.3100\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 109s 80ms/step - loss: 1.4547 - accuracy: 0.4479 - val_loss: 1.3804 - val_accuracy: 0.5100\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 109s 80ms/step - loss: 1.2850 - accuracy: 0.5678 - val_loss: 1.2124 - val_accuracy: 0.6017\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 109s 80ms/step - loss: 1.1220 - accuracy: 0.6269 - val_loss: 1.0804 - val_accuracy: 0.6018\n",
      "Epoch 5/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.9916 - accuracy: 0.6723 - val_loss: 0.9737 - val_accuracy: 0.6716\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 109s 80ms/step - loss: 0.8861 - accuracy: 0.7121 - val_loss: 0.8903 - val_accuracy: 0.7073\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.7989 - accuracy: 0.7451 - val_loss: 0.8247 - val_accuracy: 0.7202\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.7246 - accuracy: 0.7711 - val_loss: 0.7686 - val_accuracy: 0.7536\n",
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.6600 - accuracy: 0.7950 - val_loss: 0.7212 - val_accuracy: 0.7671\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.6042 - accuracy: 0.8162 - val_loss: 0.6791 - val_accuracy: 0.7739\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.5542 - accuracy: 0.8330 - val_loss: 0.6473 - val_accuracy: 0.7864\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.5099 - accuracy: 0.8488 - val_loss: 0.6151 - val_accuracy: 0.7963\n",
      "Epoch 13/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.4702 - accuracy: 0.8617 - val_loss: 0.5925 - val_accuracy: 0.8115\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.4348 - accuracy: 0.8746 - val_loss: 0.5673 - val_accuracy: 0.8140\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.4030 - accuracy: 0.8857 - val_loss: 0.5516 - val_accuracy: 0.8129\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 109s 80ms/step - loss: 0.3729 - accuracy: 0.8950 - val_loss: 0.5350 - val_accuracy: 0.8178\n",
      "Epoch 17/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.3470 - accuracy: 0.9030 - val_loss: 0.5267 - val_accuracy: 0.8233\n",
      "Epoch 18/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.3225 - accuracy: 0.9119 - val_loss: 0.5057 - val_accuracy: 0.8294\n",
      "Epoch 19/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.3005 - accuracy: 0.9175 - val_loss: 0.4921 - val_accuracy: 0.8373\n",
      "Epoch 20/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.2800 - accuracy: 0.9249 - val_loss: 0.4820 - val_accuracy: 0.8387\n",
      "Epoch 21/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.2618 - accuracy: 0.9297 - val_loss: 0.4786 - val_accuracy: 0.8365\n",
      "Epoch 22/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.2448 - accuracy: 0.9355 - val_loss: 0.4661 - val_accuracy: 0.8428\n",
      "Epoch 23/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.2286 - accuracy: 0.9395 - val_loss: 0.4649 - val_accuracy: 0.8426\n",
      "Epoch 24/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.2147 - accuracy: 0.9440 - val_loss: 0.4607 - val_accuracy: 0.8412\n",
      "Epoch 25/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 0.2007 - accuracy: 0.9488 - val_loss: 0.4506 - val_accuracy: 0.8449\n",
      "####################################################\n",
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 109s 79ms/step - loss: 1.5549 - accuracy: 0.2778 - val_loss: 1.5171 - val_accuracy: 0.2983\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.4578 - accuracy: 0.4341 - val_loss: 1.3827 - val_accuracy: 0.5045\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.2897 - accuracy: 0.5623 - val_loss: 1.2205 - val_accuracy: 0.5749\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 1.1243 - accuracy: 0.6228 - val_loss: 1.0887 - val_accuracy: 0.6188\n",
      "Epoch 5/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.9913 - accuracy: 0.6684 - val_loss: 0.9907 - val_accuracy: 0.6350\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.8849 - accuracy: 0.7101 - val_loss: 0.9123 - val_accuracy: 0.6996\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.7970 - accuracy: 0.7434 - val_loss: 0.8433 - val_accuracy: 0.7186\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.7221 - accuracy: 0.7698 - val_loss: 0.7912 - val_accuracy: 0.7294\n",
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 104s 76ms/step - loss: 0.6569 - accuracy: 0.7951 - val_loss: 0.7405 - val_accuracy: 0.7553\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.6008 - accuracy: 0.8154 - val_loss: 0.7067 - val_accuracy: 0.7684\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.5511 - accuracy: 0.8340 - val_loss: 0.6677 - val_accuracy: 0.7796\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.5060 - accuracy: 0.8496 - val_loss: 0.6384 - val_accuracy: 0.7864\n",
      "Epoch 13/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.4666 - accuracy: 0.8635 - val_loss: 0.6139 - val_accuracy: 0.7961\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.4310 - accuracy: 0.8749 - val_loss: 0.5942 - val_accuracy: 0.8047\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.3988 - accuracy: 0.8858 - val_loss: 0.5735 - val_accuracy: 0.8071\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.3695 - accuracy: 0.8949 - val_loss: 0.5579 - val_accuracy: 0.8108\n",
      "Epoch 17/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.3430 - accuracy: 0.9035 - val_loss: 0.5452 - val_accuracy: 0.8134\n",
      "Epoch 18/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.3183 - accuracy: 0.9107 - val_loss: 0.5409 - val_accuracy: 0.8083\n",
      "Epoch 19/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.2961 - accuracy: 0.9187 - val_loss: 0.5205 - val_accuracy: 0.8194\n",
      "Epoch 20/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.2765 - accuracy: 0.9238 - val_loss: 0.5050 - val_accuracy: 0.8284\n",
      "Epoch 21/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.2574 - accuracy: 0.9310 - val_loss: 0.4962 - val_accuracy: 0.8288\n",
      "Epoch 22/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.2401 - accuracy: 0.9362 - val_loss: 0.4980 - val_accuracy: 0.8272\n",
      "####################################################\n",
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.5555 - accuracy: 0.2896 - val_loss: 1.5309 - val_accuracy: 0.3841\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.4673 - accuracy: 0.4439 - val_loss: 1.4100 - val_accuracy: 0.4598\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.3090 - accuracy: 0.5415 - val_loss: 1.2549 - val_accuracy: 0.5876\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.1467 - accuracy: 0.6144 - val_loss: 1.1211 - val_accuracy: 0.6008\n",
      "Epoch 5/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.0112 - accuracy: 0.6625 - val_loss: 1.0160 - val_accuracy: 0.6460\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.9012 - accuracy: 0.7047 - val_loss: 0.9324 - val_accuracy: 0.6794\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.8072 - accuracy: 0.7401 - val_loss: 0.8590 - val_accuracy: 0.6964\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.7272 - accuracy: 0.7710 - val_loss: 0.8024 - val_accuracy: 0.7176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.6586 - accuracy: 0.7963 - val_loss: 0.7587 - val_accuracy: 0.7382\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.5998 - accuracy: 0.8159 - val_loss: 0.7072 - val_accuracy: 0.7547\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.5484 - accuracy: 0.8327 - val_loss: 0.6736 - val_accuracy: 0.7753\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.5031 - accuracy: 0.8480 - val_loss: 0.6402 - val_accuracy: 0.7819\n",
      "Epoch 13/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.4628 - accuracy: 0.8616 - val_loss: 0.6167 - val_accuracy: 0.7859\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.4269 - accuracy: 0.8729 - val_loss: 0.6030 - val_accuracy: 0.7850\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.3946 - accuracy: 0.8834 - val_loss: 0.5740 - val_accuracy: 0.8009\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.3648 - accuracy: 0.8933 - val_loss: 0.5532 - val_accuracy: 0.8069\n",
      "Epoch 17/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.3384 - accuracy: 0.9021 - val_loss: 0.5392 - val_accuracy: 0.8162\n",
      "Epoch 18/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.3143 - accuracy: 0.9107 - val_loss: 0.5317 - val_accuracy: 0.8090\n",
      "Epoch 19/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.2927 - accuracy: 0.9171 - val_loss: 0.5151 - val_accuracy: 0.8192\n",
      "Epoch 20/25\n",
      "1372/1372 [==============================] - 106s 77ms/step - loss: 0.2723 - accuracy: 0.9239 - val_loss: 0.5025 - val_accuracy: 0.8258\n",
      "Epoch 21/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.2540 - accuracy: 0.9295 - val_loss: 0.4959 - val_accuracy: 0.8283\n",
      "Epoch 22/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.2369 - accuracy: 0.9358 - val_loss: 0.4949 - val_accuracy: 0.8230\n",
      "Epoch 23/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.2219 - accuracy: 0.9402 - val_loss: 0.4840 - val_accuracy: 0.8294\n",
      "Epoch 24/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.2070 - accuracy: 0.9450 - val_loss: 0.4747 - val_accuracy: 0.8361\n",
      "Epoch 25/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1940 - accuracy: 0.9492 - val_loss: 0.4677 - val_accuracy: 0.8356\n",
      "####################################################\n",
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.5540 - accuracy: 0.2766 - val_loss: 1.5276 - val_accuracy: 0.3831\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.4681 - accuracy: 0.4191 - val_loss: 1.4032 - val_accuracy: 0.5236\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.3131 - accuracy: 0.5503 - val_loss: 1.2404 - val_accuracy: 0.6011\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.1488 - accuracy: 0.6181 - val_loss: 1.1001 - val_accuracy: 0.6343\n",
      "Epoch 5/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.0121 - accuracy: 0.6645 - val_loss: 0.9917 - val_accuracy: 0.6566\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.9029 - accuracy: 0.7037 - val_loss: 0.9040 - val_accuracy: 0.6935\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.8127 - accuracy: 0.7369 - val_loss: 0.8355 - val_accuracy: 0.7237\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.7354 - accuracy: 0.7674 - val_loss: 0.7767 - val_accuracy: 0.7397\n",
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.6692 - accuracy: 0.7924 - val_loss: 0.7278 - val_accuracy: 0.7560\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.6114 - accuracy: 0.8126 - val_loss: 0.6937 - val_accuracy: 0.7684\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 109s 80ms/step - loss: 0.5604 - accuracy: 0.8293 - val_loss: 0.6552 - val_accuracy: 0.7884\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 115s 84ms/step - loss: 0.5158 - accuracy: 0.8460 - val_loss: 0.6233 - val_accuracy: 0.7917\n",
      "Epoch 13/25\n",
      "1372/1372 [==============================] - 114s 83ms/step - loss: 0.4752 - accuracy: 0.8597 - val_loss: 0.5974 - val_accuracy: 0.8057\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 115s 84ms/step - loss: 0.4398 - accuracy: 0.8717 - val_loss: 0.5736 - val_accuracy: 0.8130\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 114s 83ms/step - loss: 0.4065 - accuracy: 0.8831 - val_loss: 0.5573 - val_accuracy: 0.8114\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 115s 84ms/step - loss: 0.3770 - accuracy: 0.8918 - val_loss: 0.5383 - val_accuracy: 0.8190\n",
      "Epoch 17/25\n",
      "1372/1372 [==============================] - 113s 82ms/step - loss: 0.3503 - accuracy: 0.9004 - val_loss: 0.5201 - val_accuracy: 0.8304\n",
      "Epoch 18/25\n",
      "1372/1372 [==============================] - 113s 82ms/step - loss: 0.3257 - accuracy: 0.9084 - val_loss: 0.5073 - val_accuracy: 0.8284\n",
      "Epoch 19/25\n",
      "1372/1372 [==============================] - 114s 83ms/step - loss: 0.3028 - accuracy: 0.9169 - val_loss: 0.4971 - val_accuracy: 0.8351\n",
      "Epoch 20/25\n",
      "1372/1372 [==============================] - 115s 84ms/step - loss: 0.2820 - accuracy: 0.9227 - val_loss: 0.4890 - val_accuracy: 0.8334\n",
      "Epoch 21/25\n",
      "1372/1372 [==============================] - 114s 83ms/step - loss: 0.2626 - accuracy: 0.9290 - val_loss: 0.4753 - val_accuracy: 0.8389\n",
      "Epoch 22/25\n",
      "1372/1372 [==============================] - 112s 82ms/step - loss: 0.2452 - accuracy: 0.9347 - val_loss: 0.4694 - val_accuracy: 0.8430\n",
      "Epoch 23/25\n",
      "1372/1372 [==============================] - 113s 83ms/step - loss: 0.2290 - accuracy: 0.9396 - val_loss: 0.4591 - val_accuracy: 0.8459\n",
      "Epoch 24/25\n",
      "1372/1372 [==============================] - 113s 82ms/step - loss: 0.2146 - accuracy: 0.9439 - val_loss: 0.4584 - val_accuracy: 0.8450\n",
      "Epoch 25/25\n",
      "1372/1372 [==============================] - 113s 83ms/step - loss: 0.2002 - accuracy: 0.9484 - val_loss: 0.4542 - val_accuracy: 0.8445\n",
      "####################################################\n",
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 114s 83ms/step - loss: 1.5536 - accuracy: 0.2871 - val_loss: 1.5276 - val_accuracy: 0.3148\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 113s 83ms/step - loss: 1.4657 - accuracy: 0.4263 - val_loss: 1.4080 - val_accuracy: 0.4672\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 111s 81ms/step - loss: 1.3095 - accuracy: 0.5348 - val_loss: 1.2493 - val_accuracy: 0.5876\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 114s 83ms/step - loss: 1.1458 - accuracy: 0.6193 - val_loss: 1.1124 - val_accuracy: 0.6118\n",
      "Epoch 5/25\n",
      "1372/1372 [==============================] - 113s 82ms/step - loss: 1.0097 - accuracy: 0.6713 - val_loss: 1.0071 - val_accuracy: 0.6349\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 113s 83ms/step - loss: 0.8996 - accuracy: 0.7107 - val_loss: 0.9182 - val_accuracy: 0.6823\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 113s 82ms/step - loss: 0.8082 - accuracy: 0.7436 - val_loss: 0.8485 - val_accuracy: 0.7123\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 67s 49ms/step - loss: 0.7314 - accuracy: 0.7717 - val_loss: 0.7892 - val_accuracy: 0.7340\n",
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 77s 56ms/step - loss: 0.6652 - accuracy: 0.7959 - val_loss: 0.7410 - val_accuracy: 0.7552\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 74s 54ms/step - loss: 0.6087 - accuracy: 0.8154 - val_loss: 0.7007 - val_accuracy: 0.7715\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 76s 55ms/step - loss: 0.5583 - accuracy: 0.8322 - val_loss: 0.6668 - val_accuracy: 0.7780\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 76s 55ms/step - loss: 0.5139 - accuracy: 0.8489 - val_loss: 0.6367 - val_accuracy: 0.7892\n",
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1372 [==============================] - 76s 56ms/step - loss: 0.4745 - accuracy: 0.8611 - val_loss: 0.6116 - val_accuracy: 0.7976\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 75s 54ms/step - loss: 0.4388 - accuracy: 0.8729 - val_loss: 0.5943 - val_accuracy: 0.8047\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 76s 55ms/step - loss: 0.4067 - accuracy: 0.8843 - val_loss: 0.5714 - val_accuracy: 0.8086\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 75s 54ms/step - loss: 0.3773 - accuracy: 0.8934 - val_loss: 0.5575 - val_accuracy: 0.8091\n",
      "Epoch 17/25\n",
      "1372/1372 [==============================] - 75s 54ms/step - loss: 0.3506 - accuracy: 0.9010 - val_loss: 0.5391 - val_accuracy: 0.8181\n",
      "Epoch 18/25\n",
      "1372/1372 [==============================] - 75s 55ms/step - loss: 0.3263 - accuracy: 0.9103 - val_loss: 0.5383 - val_accuracy: 0.8094\n",
      "Epoch 19/25\n",
      "1372/1372 [==============================] - 75s 55ms/step - loss: 0.3043 - accuracy: 0.9170 - val_loss: 0.5147 - val_accuracy: 0.8242\n",
      "Epoch 20/25\n",
      "1372/1372 [==============================] - 76s 55ms/step - loss: 0.2837 - accuracy: 0.9233 - val_loss: 0.5060 - val_accuracy: 0.8257\n",
      "Epoch 21/25\n",
      "1372/1372 [==============================] - 76s 55ms/step - loss: 0.2650 - accuracy: 0.9288 - val_loss: 0.4958 - val_accuracy: 0.8325\n",
      "Epoch 22/25\n",
      "1372/1372 [==============================] - 76s 55ms/step - loss: 0.2477 - accuracy: 0.9342 - val_loss: 0.4869 - val_accuracy: 0.8346\n",
      "Epoch 23/25\n",
      "1372/1372 [==============================] - 78s 57ms/step - loss: 0.2317 - accuracy: 0.9390 - val_loss: 0.4814 - val_accuracy: 0.8353\n",
      "Epoch 24/25\n",
      "1372/1372 [==============================] - 77s 56ms/step - loss: 0.2171 - accuracy: 0.9436 - val_loss: 0.4818 - val_accuracy: 0.8329\n",
      "####################################################\n"
     ]
    }
   ],
   "source": [
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "# Fast Text\n",
    "def doAddFastText(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"ff_0\"] = pred_train[:,0]\n",
    "    X_train[\"ff_1\"] = pred_train[:,1]\n",
    "    X_train[\"ff_2\"] = pred_train[:,2]\n",
    "    X_train[\"ff_3\"] = pred_train[:,3]\n",
    "    X_train[\"ff_4\"] = pred_train[:,4]\n",
    "    X_test[\"ff_0\"] = pred_test[:,0]\n",
    "    X_test[\"ff_1\"] = pred_test[:,1]\n",
    "    X_test[\"ff_2\"] = pred_test[:,2]\n",
    "    X_test[\"ff_3\"] = pred_test[:,3]\n",
    "    X_test[\"ff_4\"] = pred_test[:,4]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initFastText(embedding_dims,input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def preprocessFastText(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(';:,.?!\\'“”‘’\\\"')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text\n",
    "\n",
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocessFastText(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def doFastText(X_train,X_test,Y_train):\n",
    "    min_count = 2\n",
    "\n",
    "    docs = create_docs(X_train)\n",
    "    tokenizer = Tokenizer(lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "    maxlen = max([max(len(l) for l in docs)])\n",
    "\n",
    "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "    input_dim = np.max(docs) + 1\n",
    "    embedding_dims = 20\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "\n",
    "    docs_test = create_docs(X_test)\n",
    "    docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "    xtrain_pad = docs\n",
    "    xtest_pad = docs_test\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=32143233)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 5])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initFastText(embedding_dims,input_dim)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=40, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(docs_test)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        print('#############################################################################################################')\n",
    "    return doAddFastText(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "X_train,X_test = doFastText(X_train,X_test,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 54879 texts.\n",
      "Found 19617 texts.\n",
      "Found 52997 unique tokens.\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 31s 22ms/step - loss: 1.0472 - accuracy: 0.5797 - val_loss: 0.7529 - val_accuracy: 0.7293\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 31s 23ms/step - loss: 0.6681 - accuracy: 0.7526 - val_loss: 0.6703 - val_accuracy: 0.7526\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 35s 25ms/step - loss: 0.5410 - accuracy: 0.8000 - val_loss: 0.6723 - val_accuracy: 0.7553\n",
      "####################################################\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 30s 22ms/step - loss: 1.0450 - accuracy: 0.5792 - val_loss: 0.7491 - val_accuracy: 0.7226\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 31s 23ms/step - loss: 0.6692 - accuracy: 0.7501 - val_loss: 0.6636 - val_accuracy: 0.7565\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 33s 24ms/step - loss: 0.5503 - accuracy: 0.7972 - val_loss: 0.6702 - val_accuracy: 0.7549\n",
      "####################################################\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 31s 22ms/step - loss: 1.0419 - accuracy: 0.5816 - val_loss: 0.7896 - val_accuracy: 0.7010\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 35s 26ms/step - loss: 0.6729 - accuracy: 0.7492 - val_loss: 0.6999 - val_accuracy: 0.7403\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 31s 23ms/step - loss: 0.5468 - accuracy: 0.7967 - val_loss: 0.6795 - val_accuracy: 0.7472\n",
      "Epoch 4/4\n",
      "1372/1372 [==============================] - 31s 23ms/step - loss: 0.4814 - accuracy: 0.8218 - val_loss: 0.7013 - val_accuracy: 0.7459\n",
      "####################################################\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 32s 23ms/step - loss: 1.0474 - accuracy: 0.5778 - val_loss: 0.7649 - val_accuracy: 0.7175\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 32s 23ms/step - loss: 0.6697 - accuracy: 0.7506 - val_loss: 0.6547 - val_accuracy: 0.7541\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 33s 24ms/step - loss: 0.5476 - accuracy: 0.7966 - val_loss: 0.6450 - val_accuracy: 0.7626\n",
      "Epoch 4/4\n",
      "1372/1372 [==============================] - 30s 22ms/step - loss: 0.4791 - accuracy: 0.8251 - val_loss: 0.6663 - val_accuracy: 0.7558\n",
      "####################################################\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 35s 26ms/step - loss: 1.0577 - accuracy: 0.5700 - val_loss: 0.7793 - val_accuracy: 0.7082\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 31s 23ms/step - loss: 0.6610 - accuracy: 0.7569 - val_loss: 0.6898 - val_accuracy: 0.7448\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 30s 22ms/step - loss: 0.5390 - accuracy: 0.8025 - val_loss: 0.6761 - val_accuracy: 0.7534\n",
      "Epoch 4/4\n",
      "1372/1372 [==============================] - 35s 25ms/step - loss: 0.4708 - accuracy: 0.8248 - val_loss: 0.6913 - val_accuracy: 0.7530\n",
      "####################################################\n"
     ]
    }
   ],
   "source": [
    "# NN\n",
    "def doAddNN(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_0\"] = pred_train[:,0]\n",
    "    X_train[\"nn_1\"] = pred_train[:,1]\n",
    "    X_train[\"nn_2\"] = pred_train[:,2]\n",
    "    X_train[\"nn_3\"] = pred_train[:,3]\n",
    "    X_train[\"nn_4\"] = pred_train[:,4]\n",
    "    \n",
    "    X_test[\"nn_0\"] = pred_test[:,0]\n",
    "    X_test[\"nn_1\"] = pred_test[:,1]\n",
    "    X_test[\"nn_2\"] = pred_test[:,2]\n",
    "    X_test[\"nn_3\"] = pred_test[:,3]\n",
    "    X_test[\"nn_4\"] = pred_test[:,4]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN(nb_words_cnt,max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64, 5, padding='valid', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def doNN(X_train,X_test,Y_train):\n",
    "    max_len = 70\n",
    "    nb_words = 10000\n",
    "    \n",
    "    print('Processing text dataset')\n",
    "    texts_1 = []\n",
    "    for text in X_train['text']:\n",
    "        texts_1.append(text)\n",
    "    print('Found %s texts.' % len(texts_1))\n",
    "    \n",
    "    test_texts_1 = []\n",
    "    for text in X_test['text']:\n",
    "        test_texts_1.append(text)\n",
    "    print('Found %s texts.' % len(test_texts_1))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=nb_words)\n",
    "    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n",
    "    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "\n",
    "    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n",
    "    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n",
    "    del test_sequences_1\n",
    "    del sequences_1\n",
    "    nb_words_cnt = min(nb_words, len(word_index)) + 1\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=32143233)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 5])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN(nb_words_cnt,max_len)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_pad)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        print('#############################################################################################################')\n",
    "    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "X_train,X_test = doNN(X_train,X_test,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:03:09] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.43253\ttest-mlogloss:1.43250\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.49874\ttest-mlogloss:0.50836\n",
      "[40]\ttrain-mlogloss:0.38348\ttest-mlogloss:0.40617\n",
      "[60]\ttrain-mlogloss:0.35097\ttest-mlogloss:0.38739\n",
      "[80]\ttrain-mlogloss:0.33159\ttest-mlogloss:0.38140\n",
      "[100]\ttrain-mlogloss:0.31583\ttest-mlogloss:0.37877\n",
      "[120]\ttrain-mlogloss:0.30048\ttest-mlogloss:0.37678\n",
      "[140]\ttrain-mlogloss:0.28624\ttest-mlogloss:0.37583\n",
      "[160]\ttrain-mlogloss:0.27303\ttest-mlogloss:0.37525\n",
      "[180]\ttrain-mlogloss:0.26085\ttest-mlogloss:0.37489\n",
      "[200]\ttrain-mlogloss:0.24930\ttest-mlogloss:0.37444\n",
      "[220]\ttrain-mlogloss:0.23879\ttest-mlogloss:0.37445\n",
      "[240]\ttrain-mlogloss:0.22834\ttest-mlogloss:0.37449\n",
      "[260]\ttrain-mlogloss:0.21822\ttest-mlogloss:0.37465\n",
      "[280]\ttrain-mlogloss:0.20885\ttest-mlogloss:0.37527\n",
      "Stopping. Best iteration:\n",
      "[238]\ttrain-mlogloss:0.22947\ttest-mlogloss:0.37426\n",
      "\n",
      "[23:04:39] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.43198\ttest-mlogloss:1.43817\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.49393\ttest-mlogloss:0.53291\n",
      "[40]\ttrain-mlogloss:0.37882\ttest-mlogloss:0.43052\n",
      "[60]\ttrain-mlogloss:0.34645\ttest-mlogloss:0.41032\n",
      "[80]\ttrain-mlogloss:0.32772\ttest-mlogloss:0.40425\n",
      "[100]\ttrain-mlogloss:0.31103\ttest-mlogloss:0.40103\n",
      "[120]\ttrain-mlogloss:0.29646\ttest-mlogloss:0.39932\n",
      "[140]\ttrain-mlogloss:0.28298\ttest-mlogloss:0.39844\n",
      "[160]\ttrain-mlogloss:0.26959\ttest-mlogloss:0.39792\n",
      "[180]\ttrain-mlogloss:0.25804\ttest-mlogloss:0.39737\n",
      "[200]\ttrain-mlogloss:0.24662\ttest-mlogloss:0.39699\n",
      "[220]\ttrain-mlogloss:0.23511\ttest-mlogloss:0.39681\n",
      "[240]\ttrain-mlogloss:0.22516\ttest-mlogloss:0.39683\n",
      "[260]\ttrain-mlogloss:0.21572\ttest-mlogloss:0.39656\n",
      "[280]\ttrain-mlogloss:0.20649\ttest-mlogloss:0.39675\n",
      "[300]\ttrain-mlogloss:0.19788\ttest-mlogloss:0.39694\n",
      "Stopping. Best iteration:\n",
      "[268]\ttrain-mlogloss:0.21218\ttest-mlogloss:0.39633\n",
      "\n",
      "[23:06:18] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.43198\ttest-mlogloss:1.43287\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.49436\ttest-mlogloss:0.52090\n",
      "[40]\ttrain-mlogloss:0.37826\ttest-mlogloss:0.42494\n",
      "[60]\ttrain-mlogloss:0.34657\ttest-mlogloss:0.40821\n",
      "[80]\ttrain-mlogloss:0.32745\ttest-mlogloss:0.40305\n",
      "[100]\ttrain-mlogloss:0.31068\ttest-mlogloss:0.39990\n",
      "[120]\ttrain-mlogloss:0.29569\ttest-mlogloss:0.39825\n",
      "[140]\ttrain-mlogloss:0.28151\ttest-mlogloss:0.39745\n",
      "[160]\ttrain-mlogloss:0.26835\ttest-mlogloss:0.39652\n",
      "[180]\ttrain-mlogloss:0.25614\ttest-mlogloss:0.39642\n",
      "[200]\ttrain-mlogloss:0.24535\ttest-mlogloss:0.39688\n",
      "Stopping. Best iteration:\n",
      "[169]\ttrain-mlogloss:0.26289\ttest-mlogloss:0.39630\n",
      "\n",
      "[23:07:27] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.43267\ttest-mlogloss:1.43156\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.49907\ttest-mlogloss:0.50585\n",
      "[40]\ttrain-mlogloss:0.38405\ttest-mlogloss:0.40419\n",
      "[60]\ttrain-mlogloss:0.35145\ttest-mlogloss:0.38541\n",
      "[80]\ttrain-mlogloss:0.33272\ttest-mlogloss:0.37951\n",
      "[100]\ttrain-mlogloss:0.31576\ttest-mlogloss:0.37692\n",
      "[120]\ttrain-mlogloss:0.30189\ttest-mlogloss:0.37525\n",
      "[140]\ttrain-mlogloss:0.28808\ttest-mlogloss:0.37479\n",
      "[160]\ttrain-mlogloss:0.27486\ttest-mlogloss:0.37449\n",
      "[180]\ttrain-mlogloss:0.26288\ttest-mlogloss:0.37439\n",
      "[200]\ttrain-mlogloss:0.25148\ttest-mlogloss:0.37471\n",
      "[220]\ttrain-mlogloss:0.24108\ttest-mlogloss:0.37483\n",
      "Stopping. Best iteration:\n",
      "[182]\ttrain-mlogloss:0.26165\ttest-mlogloss:0.37430\n",
      "\n",
      "[23:08:40] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.43958\ttest-mlogloss:1.44412\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.49132\ttest-mlogloss:0.52728\n",
      "[40]\ttrain-mlogloss:0.37783\ttest-mlogloss:0.43227\n",
      "[60]\ttrain-mlogloss:0.34579\ttest-mlogloss:0.41610\n",
      "[80]\ttrain-mlogloss:0.32638\ttest-mlogloss:0.41026\n",
      "[100]\ttrain-mlogloss:0.30945\ttest-mlogloss:0.40766\n",
      "[120]\ttrain-mlogloss:0.29509\ttest-mlogloss:0.40582\n",
      "[140]\ttrain-mlogloss:0.28161\ttest-mlogloss:0.40560\n",
      "[160]\ttrain-mlogloss:0.26879\ttest-mlogloss:0.40532\n",
      "[180]\ttrain-mlogloss:0.25717\ttest-mlogloss:0.40527\n",
      "[200]\ttrain-mlogloss:0.24546\ttest-mlogloss:0.40526\n",
      "[220]\ttrain-mlogloss:0.23479\ttest-mlogloss:0.40577\n",
      "Stopping. Best iteration:\n",
      "[187]\ttrain-mlogloss:0.25307\ttest-mlogloss:0.40502\n",
      "\n",
      "cv scores :  [0.374254669870907, 0.39632741185780007, 0.39629713994992327, 0.37429750360830943, 0.40502376285223635]\n"
     ]
    }
   ],
   "source": [
    "# Final Model\n",
    "# XGBoost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 5\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 5\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    drop_columns=[\"text\",\"words\"]\n",
    "    x_train = X_train.drop(drop_columns+['author'],axis=1)\n",
    "    x_test = X_test.drop(drop_columns,axis=1)\n",
    "    y_train = Y_train\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=32143233)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([x_train.shape[0], 5])\n",
    "    for dev_index, val_index in kf.split(x_train):\n",
    "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    return pred_full_test/5\n",
    "result = do(X_train,X_test,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cv scores :  [0.37971865148208395, 0.3913440671935568, 0.39145263387349694, 0.38940780721717677, 0.38413344802020066]\n",
    "##### cv scores :  [0.37639503734384844, 0.38710317050095433, 0.3945025033210634, 0.3729838736870731, 0.3990754037926082]\n",
    "##### cv scores :  [0.374254669870907, 0.39632741185780007, 0.39629713994992327, 0.37429750360830943, 0.40502376285223635]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.789531</td>\n",
       "      <td>0.197901</td>\n",
       "      <td>0.007342</td>\n",
       "      <td>0.003493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.995005</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.002186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.014466</td>\n",
       "      <td>0.983473</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.001260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.983480</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>0.002845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19612</th>\n",
       "      <td>19612</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.999509</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19613</th>\n",
       "      <td>19613</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.996552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19614</th>\n",
       "      <td>19614</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.999222</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19615</th>\n",
       "      <td>19615</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.999317</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19616</th>\n",
       "      <td>19616</td>\n",
       "      <td>0.994948</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.003440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19617 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index         0         1         2         3         4\n",
       "0          0  0.001733  0.789531  0.197901  0.007342  0.003493\n",
       "1          1  0.002087  0.995005  0.000231  0.000491  0.002186\n",
       "2          2  0.999280  0.000339  0.000053  0.000043  0.000284\n",
       "3          3  0.000296  0.014466  0.983473  0.000505  0.001260\n",
       "4          4  0.983480  0.005917  0.002239  0.005519  0.002845\n",
       "...      ...       ...       ...       ...       ...       ...\n",
       "19612  19612  0.000301  0.999509  0.000070  0.000091  0.000029\n",
       "19613  19613  0.002426  0.000237  0.000513  0.000272  0.996552\n",
       "19614  19614  0.000349  0.999222  0.000136  0.000189  0.000104\n",
       "19615  19615  0.000135  0.999317  0.000245  0.000252  0.000051\n",
       "19616  19616  0.994948  0.000576  0.000657  0.000378  0.003440\n",
       "\n",
       "[19617 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission=pd.read_csv('open/sample_submission.csv', encoding='utf-8')\n",
    "sample_submission[['0', '1', '2', '3', '4']] = result\n",
    "sample_submission.to_csv(\"kg_4_1123.csv\", index=False)\n",
    "sample_submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
