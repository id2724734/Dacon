{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import sys\n",
    "import glob\n",
    "import errno\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk.data\n",
    "import nltk\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from subprocess import check_call\n",
    "from shutil import copyfile\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# import mpld3\n",
    "# mpld3.enable_notebook()\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalAveragePooling1D,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from sklearn.linear_model import SGDClassifier as sgd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "X_train = pd.read_csv('open/train.csv', header=0,delimiter=\",\" )\n",
    "X_test = pd.read_csv( 'open/test_x.csv', header=0,delimiter=\",\" )\n",
    "\n",
    "authors = [0,1,2,3,4]\n",
    "Y_train = LabelEncoder().fit_transform(X_train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "def clean(X_train,X_test):\n",
    "    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n",
    "    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n",
    "    return X_train,X_test\n",
    "X_train,X_test = clean(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation\n",
    "punctuations = [{\"id\":1,\"p\":\"[;:]\"},\n",
    "                {\"id\":2,\"p\":\"[,.]\"},\n",
    "                {\"id\":3,\"p\":\"[?]\"},\n",
    "                {\"id\":4,\"p\":\"[!]\"},\n",
    "                {\"id\":5,\"p\":\"[\\']\"},\n",
    "                {\"id\":6,\"p\":\"[\\\"]\"},\n",
    "                {\"id\":7,\"p\":\"[;:,.?!\\'\\\"]\"}]\n",
    "for p in punctuations:\n",
    "    punctuation = p[\"p\"]\n",
    "    _train =  [ sentence.split() for sentence in X_train['text'] ]\n",
    "    X_train['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _train]    \n",
    "\n",
    "    _test =  [ sentence.split() for sentence in X_test['text'] ]\n",
    "    X_test['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _test]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  1.206228258630992\n"
     ]
    }
   ],
   "source": [
    "# tfidf - words - nb\n",
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf,full_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_tfidf_MNB(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 5])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_tfidf_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_words_nb_0\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_words_nb_1\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_words_nb_2\"] = pred_train[:,2]\n",
    "X_train[\"tfidf_words_nb_3\"] = pred_train[:,3]\n",
    "X_train[\"tfidf_words_nb_4\"] = pred_train[:,4]\n",
    "X_test[\"tfidf_words_nb_0\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_words_nb_1\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_words_nb_2\"] = pred_test[:,2]\n",
    "X_test[\"tfidf_words_nb_3\"] = pred_test[:,3]\n",
    "X_test[\"tfidf_words_nb_4\"] = pred_test[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  1.8069104479701459\n"
     ]
    }
   ],
   "source": [
    "# tfidf - chars - nb\n",
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 5])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "pred_train,pred_test = do(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_chars_nb_0\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_chars_nb_1\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_chars_nb_2\"] = pred_train[:,2]\n",
    "X_train[\"tfidf_chars_nb_3\"] = pred_train[:,3]\n",
    "X_train[\"tfidf_chars_nb_4\"] = pred_train[:,4]\n",
    "X_test[\"tfidf_chars_nb_0\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_chars_nb_1\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_chars_nb_2\"] = pred_test[:,2]\n",
    "X_test[\"tfidf_chars_nb_3\"] = pred_test[:,3]\n",
    "X_test[\"tfidf_chars_nb_4\"] = pred_test[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  1.1789477530668817\n"
     ]
    }
   ],
   "source": [
    "def countWords(X_train,X_test):\n",
    "    count_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 5])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_words_nb_0\"] = pred_train[:,0]\n",
    "X_train[\"count_words_nb_1\"] = pred_train[:,1]\n",
    "X_train[\"count_words_nb_2\"] = pred_train[:,2]\n",
    "X_train[\"count_words_nb_3\"] = pred_train[:,3]\n",
    "X_train[\"count_words_nb_4\"] = pred_train[:,4]\n",
    "X_test[\"count_words_nb_0\"] = pred_test[:,0]\n",
    "X_test[\"count_words_nb_1\"] = pred_test[:,1]\n",
    "X_test[\"count_words_nb_2\"] = pred_test[:,2]\n",
    "X_test[\"count_words_nb_3\"] = pred_test[:,3]\n",
    "X_test[\"count_words_nb_4\"] = pred_test[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  5.806984089402464\n"
     ]
    }
   ],
   "source": [
    "# count - chars - nb\n",
    "def countChars(X_train,X_test):\n",
    "    count_vec = CountVectorizer(ngram_range=(1,7),analyzer='char')\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_chars_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countChars(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 5])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_chars_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_chars_nb_0\"] = pred_train[:,0]\n",
    "X_train[\"count_chars_nb_1\"] = pred_train[:,1]\n",
    "X_train[\"count_chars_nb_2\"] = pred_train[:,2]\n",
    "X_train[\"count_chars_nb_3\"] = pred_train[:,3]\n",
    "X_train[\"count_chars_nb_4\"] = pred_train[:,4]\n",
    "\n",
    "X_test[\"count_chars_nb_0\"] = pred_test[:,0]\n",
    "X_test[\"count_chars_nb_1\"] = pred_test[:,1]\n",
    "X_test[\"count_chars_nb_2\"] = pred_test[:,2]\n",
    "X_test[\"count_chars_nb_3\"] = pred_test[:,3]\n",
    "X_test[\"count_chars_nb_4\"] = pred_test[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 3/54879 [00:00<30:33, 29.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 54879/54879 [24:14<00:00, 37.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 19617/19617 [18:16<00:00, 17.89it/s]\n"
     ]
    }
   ],
   "source": [
    "wv = 'open/glove.6B.100d.txt'\n",
    "def loadWordVecs():\n",
    "    embeddings_index = {}\n",
    "    f = open(wv, encoding = 'utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def doGlove(x_train,x_test):\n",
    "    embeddings_index = loadWordVecs()\n",
    "    # create sentence vectors using the above function for training and validation set\n",
    "    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n",
    "    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n",
    "    xtrain_glove = np.array(xtrain_glove)\n",
    "    xtest_glove = np.array(xtest_glove)\n",
    "    return xtrain_glove,xtest_glove,embeddings_index\n",
    "\n",
    "glove_vecs_train,glove_vecs_test,embeddings_index = doGlove(X_train['text'],X_test['text'])\n",
    "X_train[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_train.tolist())\n",
    "X_test[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.5135 - accuracy: 0.3480 - val_loss: 1.4208 - val_accuracy: 0.4515\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 106s 77ms/step - loss: 1.2441 - accuracy: 0.5824 - val_loss: 1.1198 - val_accuracy: 0.6204\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.9624 - accuracy: 0.6914 - val_loss: 0.9167 - val_accuracy: 0.6983\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.7770 - accuracy: 0.7570 - val_loss: 0.7917 - val_accuracy: 0.7394\n",
      "Epoch 5/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.6479 - accuracy: 0.8017 - val_loss: 0.7058 - val_accuracy: 0.7711\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.5487 - accuracy: 0.8361 - val_loss: 0.6449 - val_accuracy: 0.7849\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.4693 - accuracy: 0.8607 - val_loss: 0.5962 - val_accuracy: 0.8005\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.4049 - accuracy: 0.8826 - val_loss: 0.5601 - val_accuracy: 0.8132\n",
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.3506 - accuracy: 0.9006 - val_loss: 0.5299 - val_accuracy: 0.8203\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.3054 - accuracy: 0.9144 - val_loss: 0.5085 - val_accuracy: 0.8245\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.2665 - accuracy: 0.9272 - val_loss: 0.4931 - val_accuracy: 0.8289\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.2330 - accuracy: 0.9383 - val_loss: 0.4802 - val_accuracy: 0.8317\n",
      "Epoch 13/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.2045 - accuracy: 0.9471 - val_loss: 0.4680 - val_accuracy: 0.8374\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1799 - accuracy: 0.9549 - val_loss: 0.4638 - val_accuracy: 0.8363\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1585 - accuracy: 0.9605 - val_loss: 0.4570 - val_accuracy: 0.8397\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1400 - accuracy: 0.9661 - val_loss: 0.4526 - val_accuracy: 0.8409\n",
      "Epoch 17/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1238 - accuracy: 0.9708 - val_loss: 0.4524 - val_accuracy: 0.8425\n",
      "Epoch 18/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1098 - accuracy: 0.9752 - val_loss: 0.4549 - val_accuracy: 0.8404\n",
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.5053 - accuracy: 0.3807 - val_loss: 1.3869 - val_accuracy: 0.4892\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 1.2128 - accuracy: 0.5764 - val_loss: 1.0985 - val_accuracy: 0.6255\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.9479 - accuracy: 0.6885 - val_loss: 0.9183 - val_accuracy: 0.6965\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.7705 - accuracy: 0.7561 - val_loss: 0.8014 - val_accuracy: 0.7389\n",
      "Epoch 5/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.6426 - accuracy: 0.8028 - val_loss: 0.7171 - val_accuracy: 0.7635\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.5441 - accuracy: 0.8368 - val_loss: 0.6545 - val_accuracy: 0.7816\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.4649 - accuracy: 0.8636 - val_loss: 0.6078 - val_accuracy: 0.7921\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.4003 - accuracy: 0.8838 - val_loss: 0.5710 - val_accuracy: 0.8088\n",
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.3464 - accuracy: 0.9015 - val_loss: 0.5427 - val_accuracy: 0.8143\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.3014 - accuracy: 0.9167 - val_loss: 0.5198 - val_accuracy: 0.8207\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 106s 77ms/step - loss: 0.2628 - accuracy: 0.9290 - val_loss: 0.5065 - val_accuracy: 0.8252\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.2301 - accuracy: 0.9392 - val_loss: 0.4895 - val_accuracy: 0.8299\n",
      "Epoch 13/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.2019 - accuracy: 0.9483 - val_loss: 0.4792 - val_accuracy: 0.8326\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1772 - accuracy: 0.9554 - val_loss: 0.4723 - val_accuracy: 0.8352\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1561 - accuracy: 0.9615 - val_loss: 0.4696 - val_accuracy: 0.8359\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1381 - accuracy: 0.9671 - val_loss: 0.4648 - val_accuracy: 0.8375\n",
      "Epoch 17/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.1218 - accuracy: 0.9718 - val_loss: 0.4696 - val_accuracy: 0.8360\n",
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 1.5104 - accuracy: 0.3599 - val_loss: 1.4047 - val_accuracy: 0.5617\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 1.2277 - accuracy: 0.5960 - val_loss: 1.1046 - val_accuracy: 0.6164\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.9545 - accuracy: 0.6917 - val_loss: 0.9174 - val_accuracy: 0.6981\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.7748 - accuracy: 0.7573 - val_loss: 0.7958 - val_accuracy: 0.7288\n",
      "Epoch 5/25\n",
      "1372/1372 [==============================] - 107s 78ms/step - loss: 0.6463 - accuracy: 0.8027 - val_loss: 0.7156 - val_accuracy: 0.7539\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.5479 - accuracy: 0.8372 - val_loss: 0.6517 - val_accuracy: 0.7740\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 108s 79ms/step - loss: 0.4689 - accuracy: 0.8635 - val_loss: 0.6034 - val_accuracy: 0.7935\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 108s 78ms/step - loss: 0.4040 - accuracy: 0.8850 - val_loss: 0.5662 - val_accuracy: 0.8016\n",
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 109s 80ms/step - loss: 0.3500 - accuracy: 0.9014 - val_loss: 0.5392 - val_accuracy: 0.8109\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 117s 85ms/step - loss: 0.3047 - accuracy: 0.9162 - val_loss: 0.5166 - val_accuracy: 0.8183\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 117s 85ms/step - loss: 0.2662 - accuracy: 0.9285 - val_loss: 0.4996 - val_accuracy: 0.8233\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 116s 84ms/step - loss: 0.2335 - accuracy: 0.9385 - val_loss: 0.4902 - val_accuracy: 0.8281\n",
      "Epoch 13/25\n",
      "1372/1372 [==============================] - 117s 85ms/step - loss: 0.2051 - accuracy: 0.9477 - val_loss: 0.4755 - val_accuracy: 0.8330\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 116s 85ms/step - loss: 0.1808 - accuracy: 0.9554 - val_loss: 0.4714 - val_accuracy: 0.8342\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 116s 84ms/step - loss: 0.1596 - accuracy: 0.9610 - val_loss: 0.4646 - val_accuracy: 0.8360\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 119s 87ms/step - loss: 0.1414 - accuracy: 0.9664 - val_loss: 0.4669 - val_accuracy: 0.8364\n",
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 119s 86ms/step - loss: 1.5100 - accuracy: 0.3754 - val_loss: 1.3939 - val_accuracy: 0.4940\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 116s 85ms/step - loss: 1.2389 - accuracy: 0.5609 - val_loss: 1.1158 - val_accuracy: 0.6149\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 117s 85ms/step - loss: 0.9758 - accuracy: 0.6769 - val_loss: 0.9292 - val_accuracy: 0.6942\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 119s 87ms/step - loss: 0.7939 - accuracy: 0.7514 - val_loss: 0.8061 - val_accuracy: 0.7331\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1372 [==============================] - 118s 86ms/step - loss: 0.6635 - accuracy: 0.7959 - val_loss: 0.7207 - val_accuracy: 0.7665\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 116s 85ms/step - loss: 0.5626 - accuracy: 0.8317 - val_loss: 0.6575 - val_accuracy: 0.7829\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 116s 85ms/step - loss: 0.4819 - accuracy: 0.8596 - val_loss: 0.6082 - val_accuracy: 0.7984\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 118s 86ms/step - loss: 0.4165 - accuracy: 0.8817 - val_loss: 0.5705 - val_accuracy: 0.8110\n",
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 119s 87ms/step - loss: 0.3616 - accuracy: 0.8982 - val_loss: 0.5435 - val_accuracy: 0.8202\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 117s 85ms/step - loss: 0.3158 - accuracy: 0.9128 - val_loss: 0.5175 - val_accuracy: 0.8258\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 117s 85ms/step - loss: 0.2767 - accuracy: 0.9264 - val_loss: 0.5017 - val_accuracy: 0.8306\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 117s 85ms/step - loss: 0.2434 - accuracy: 0.9356 - val_loss: 0.4879 - val_accuracy: 0.8339\n",
      "Epoch 13/25\n",
      "1372/1372 [==============================] - 116s 85ms/step - loss: 0.2146 - accuracy: 0.9449 - val_loss: 0.4787 - val_accuracy: 0.8359\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 74s 54ms/step - loss: 0.1894 - accuracy: 0.9527 - val_loss: 0.4701 - val_accuracy: 0.8394\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 79s 58ms/step - loss: 0.1679 - accuracy: 0.9585 - val_loss: 0.4640 - val_accuracy: 0.8428\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 77s 56ms/step - loss: 0.1492 - accuracy: 0.9640 - val_loss: 0.4615 - val_accuracy: 0.8440\n",
      "Epoch 17/25\n",
      "1372/1372 [==============================] - 77s 56ms/step - loss: 0.1323 - accuracy: 0.9692 - val_loss: 0.4591 - val_accuracy: 0.8460\n",
      "Epoch 18/25\n",
      "1372/1372 [==============================] - 76s 56ms/step - loss: 0.1183 - accuracy: 0.9724 - val_loss: 0.4597 - val_accuracy: 0.8472\n",
      "Epoch 1/25\n",
      "1372/1372 [==============================] - 77s 56ms/step - loss: 1.5125 - accuracy: 0.3695 - val_loss: 1.4085 - val_accuracy: 0.5166\n",
      "Epoch 2/25\n",
      "1372/1372 [==============================] - 78s 57ms/step - loss: 1.2327 - accuracy: 0.5850 - val_loss: 1.1049 - val_accuracy: 0.6281\n",
      "Epoch 3/25\n",
      "1372/1372 [==============================] - 80s 58ms/step - loss: 0.9495 - accuracy: 0.6962 - val_loss: 0.9107 - val_accuracy: 0.6889\n",
      "Epoch 4/25\n",
      "1372/1372 [==============================] - 80s 58ms/step - loss: 0.7641 - accuracy: 0.7636 - val_loss: 0.7866 - val_accuracy: 0.7372\n",
      "Epoch 5/25\n",
      "1372/1372 [==============================] - 80s 58ms/step - loss: 0.6335 - accuracy: 0.8069 - val_loss: 0.7033 - val_accuracy: 0.7633\n",
      "Epoch 6/25\n",
      "1372/1372 [==============================] - 78s 57ms/step - loss: 0.5340 - accuracy: 0.8405 - val_loss: 0.6409 - val_accuracy: 0.7858\n",
      "Epoch 7/25\n",
      "1372/1372 [==============================] - 79s 58ms/step - loss: 0.4550 - accuracy: 0.8675 - val_loss: 0.5957 - val_accuracy: 0.7956\n",
      "Epoch 8/25\n",
      "1372/1372 [==============================] - 79s 57ms/step - loss: 0.3907 - accuracy: 0.8877 - val_loss: 0.5605 - val_accuracy: 0.8082\n",
      "Epoch 9/25\n",
      "1372/1372 [==============================] - 80s 58ms/step - loss: 0.3377 - accuracy: 0.9036 - val_loss: 0.5335 - val_accuracy: 0.8170\n",
      "Epoch 10/25\n",
      "1372/1372 [==============================] - 87s 63ms/step - loss: 0.2929 - accuracy: 0.9182 - val_loss: 0.5111 - val_accuracy: 0.8232\n",
      "Epoch 11/25\n",
      "1372/1372 [==============================] - 103s 75ms/step - loss: 0.2556 - accuracy: 0.9306 - val_loss: 0.4942 - val_accuracy: 0.8309\n",
      "Epoch 12/25\n",
      "1372/1372 [==============================] - 83s 61ms/step - loss: 0.2237 - accuracy: 0.9403 - val_loss: 0.4896 - val_accuracy: 0.8263\n",
      "Epoch 13/25\n",
      "1372/1372 [==============================] - 82s 60ms/step - loss: 0.1960 - accuracy: 0.9494 - val_loss: 0.4740 - val_accuracy: 0.8394\n",
      "Epoch 14/25\n",
      "1372/1372 [==============================] - 81s 59ms/step - loss: 0.1722 - accuracy: 0.9568 - val_loss: 0.4665 - val_accuracy: 0.8420\n",
      "Epoch 15/25\n",
      "1372/1372 [==============================] - 80s 59ms/step - loss: 0.1517 - accuracy: 0.9630 - val_loss: 0.4664 - val_accuracy: 0.8375\n",
      "Epoch 16/25\n",
      "1372/1372 [==============================] - 80s 58ms/step - loss: 0.1338 - accuracy: 0.9671 - val_loss: 0.4656 - val_accuracy: 0.8426\n",
      "Epoch 17/25\n",
      "1372/1372 [==============================] - 80s 58ms/step - loss: 0.1182 - accuracy: 0.9716 - val_loss: 0.4633 - val_accuracy: 0.8436\n",
      "Epoch 18/25\n",
      "1372/1372 [==============================] - 82s 60ms/step - loss: 0.1047 - accuracy: 0.9755 - val_loss: 0.4655 - val_accuracy: 0.8443\n",
      "Processing text dataset\n",
      "Found 54879 texts.\n",
      "Found 19617 texts.\n",
      "Found 52997 unique tokens.\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 36s 26ms/step - loss: 1.0523 - accuracy: 0.5751 - val_loss: 0.7708 - val_accuracy: 0.7187\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 33s 24ms/step - loss: 0.6697 - accuracy: 0.7521 - val_loss: 0.6618 - val_accuracy: 0.7600\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 40s 29ms/step - loss: 0.5440 - accuracy: 0.7990 - val_loss: 0.6674 - val_accuracy: 0.7538\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 32s 24ms/step - loss: 1.0456 - accuracy: 0.5793 - val_loss: 0.7892 - val_accuracy: 0.7111\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 38s 28ms/step - loss: 0.6694 - accuracy: 0.7522 - val_loss: 0.6891 - val_accuracy: 0.7444\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 38s 28ms/step - loss: 0.5407 - accuracy: 0.7991 - val_loss: 0.6722 - val_accuracy: 0.7516\n",
      "Epoch 4/4\n",
      "1372/1372 [==============================] - 33s 24ms/step - loss: 0.4783 - accuracy: 0.8230 - val_loss: 0.6794 - val_accuracy: 0.7546\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 41s 30ms/step - loss: 1.0442 - accuracy: 0.5812 - val_loss: 0.7484 - val_accuracy: 0.7188\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 32s 24ms/step - loss: 0.6568 - accuracy: 0.7576 - val_loss: 0.6755 - val_accuracy: 0.7474\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 38s 28ms/step - loss: 0.5328 - accuracy: 0.8032 - val_loss: 0.6616 - val_accuracy: 0.7535\n",
      "Epoch 4/4\n",
      "1372/1372 [==============================] - 38s 28ms/step - loss: 0.4664 - accuracy: 0.8289 - val_loss: 0.6759 - val_accuracy: 0.7497\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 32s 24ms/step - loss: 1.0561 - accuracy: 0.5748 - val_loss: 0.7415 - val_accuracy: 0.7285\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 41s 30ms/step - loss: 0.6544 - accuracy: 0.7605 - val_loss: 0.6739 - val_accuracy: 0.7535\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 35s 25ms/step - loss: 0.5400 - accuracy: 0.8010 - val_loss: 0.6616 - val_accuracy: 0.7570\n",
      "Epoch 4/4\n",
      "1372/1372 [==============================] - 35s 26ms/step - loss: 0.4727 - accuracy: 0.8267 - val_loss: 0.6863 - val_accuracy: 0.7565\n",
      "Epoch 1/4\n",
      "1372/1372 [==============================] - 39s 28ms/step - loss: 1.0451 - accuracy: 0.5807 - val_loss: 0.7585 - val_accuracy: 0.7258\n",
      "Epoch 2/4\n",
      "1372/1372 [==============================] - 32s 23ms/step - loss: 0.6560 - accuracy: 0.7577 - val_loss: 0.6843 - val_accuracy: 0.7497\n",
      "Epoch 3/4\n",
      "1372/1372 [==============================] - 38s 28ms/step - loss: 0.5379 - accuracy: 0.8011 - val_loss: 0.6641 - val_accuracy: 0.7541\n",
      "Epoch 4/4\n",
      "1372/1372 [==============================] - 38s 28ms/step - loss: 0.4726 - accuracy: 0.8253 - val_loss: 0.6699 - val_accuracy: 0.7569\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.4840 - val_loss: 1.2826\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.3123 - val_loss: 1.2456\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2799 - val_loss: 1.2331\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2639 - val_loss: 1.2260\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2492 - val_loss: 1.2194\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2395 - val_loss: 1.2088\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2302 - val_loss: 1.2054\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2210 - val_loss: 1.2060\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.4883 - val_loss: 1.2769\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.3150 - val_loss: 1.2444\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2811 - val_loss: 1.2270\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2663 - val_loss: 1.2146\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2510 - val_loss: 1.2085\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2397 - val_loss: 1.1993\n",
      "Epoch 7/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2287 - val_loss: 1.1956\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2235 - val_loss: 1.1907\n",
      "Epoch 9/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2161 - val_loss: 1.1872\n",
      "Epoch 10/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2091 - val_loss: 1.1845\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.4920 - val_loss: 1.2724\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.3171 - val_loss: 1.2395\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2847 - val_loss: 1.2224\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2642 - val_loss: 1.2085\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2539 - val_loss: 1.2018\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2402 - val_loss: 1.1940\n",
      "Epoch 7/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2300 - val_loss: 1.1893\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 2s 2ms/step - loss: 1.2252 - val_loss: 1.1897\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.4826 - val_loss: 1.2798\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.3155 - val_loss: 1.2367\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2822 - val_loss: 1.2198\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2625 - val_loss: 1.2102\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2473 - val_loss: 1.2013\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2390 - val_loss: 1.1930\n",
      "Epoch 7/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2298 - val_loss: 1.1926\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2200 - val_loss: 1.1859\n",
      "Epoch 9/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2189 - val_loss: 1.1863\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.4929 - val_loss: 1.2846\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.3174 - val_loss: 1.2479\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2826 - val_loss: 1.2283\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2641 - val_loss: 1.2193\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2527 - val_loss: 1.2110\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2393 - val_loss: 1.2033\n",
      "Epoch 7/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2288 - val_loss: 1.1964\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2203 - val_loss: 1.1916\n",
      "Epoch 9/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2149 - val_loss: 1.1882\n",
      "Epoch 10/10\n",
      "1372/1372 [==============================] - 3s 2ms/step - loss: 1.2076 - val_loss: 1.1851\n"
     ]
    }
   ],
   "source": [
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "# NN\n",
    "def doAddNN(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_0\"] = pred_train[:,0]\n",
    "    X_train[\"nn_1\"] = pred_train[:,1]\n",
    "    X_train[\"nn_2\"] = pred_train[:,2]\n",
    "    X_train[\"nn_3\"] = pred_train[:,3]\n",
    "    X_train[\"nn_4\"] = pred_train[:,4]\n",
    "    \n",
    "    X_test[\"nn_0\"] = pred_test[:,0]\n",
    "    X_test[\"nn_1\"] = pred_test[:,1]\n",
    "    X_test[\"nn_2\"] = pred_test[:,2]\n",
    "    X_test[\"nn_3\"] = pred_test[:,3]\n",
    "    X_test[\"nn_4\"] = pred_test[:,4]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN(nb_words_cnt,max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,\n",
    "                     5,\n",
    "                     padding='valid',\n",
    "                     activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def doNN(X_train,X_test,Y_train):\n",
    "    max_len = 70\n",
    "    nb_words = 10000\n",
    "    \n",
    "    print('Processing text dataset')\n",
    "    texts_1 = []\n",
    "    for text in X_train['text']:\n",
    "        texts_1.append(text)\n",
    "\n",
    "    print('Found %s texts.' % len(texts_1))\n",
    "    test_texts_1 = []\n",
    "    for text in X_test['text']:\n",
    "        test_texts_1.append(text)\n",
    "    print('Found %s texts.' % len(test_texts_1))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=nb_words)\n",
    "    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n",
    "    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "\n",
    "    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n",
    "    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n",
    "    del test_sequences_1\n",
    "    del sequences_1\n",
    "    nb_words_cnt = min(nb_words, len(word_index)) + 1\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 5])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN(nb_words_cnt,max_len)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_pad)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "## NN Glove\n",
    "\n",
    "def doAddNN_glove(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_glove_0\"] = pred_train[:,0]\n",
    "    X_train[\"nn_glove_1\"] = pred_train[:,1]\n",
    "    X_train[\"nn_glove_2\"] = pred_train[:,2]\n",
    "    X_train[\"nn_glove_3\"] = pred_train[:,3]\n",
    "    X_train[\"nn_glove_4\"] = pred_train[:,4]\n",
    "    X_test[\"nn_glove_0\"] = pred_test[:,0]\n",
    "    X_test[\"nn_glove_1\"] = pred_test[:,1]\n",
    "    X_test[\"nn_glove_2\"] = pred_test[:,2]\n",
    "    X_test[\"nn_glove_3\"] = pred_test[:,3]\n",
    "    X_test[\"nn_glove_4\"] = pred_test[:,4]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN_glove():\n",
    "    # create a simple 3 layer sequential neural net\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, input_dim=100, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(5))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def doNN_glove(X_train,X_test,Y_train,xtrain_glove,xtest_glove):\n",
    "    # scale the data before any neural net:\n",
    "    scl = preprocessing.StandardScaler()\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    xtrain_glove = scl.fit_transform(xtrain_glove)\n",
    "    xtest_glove = scl.fit_transform(xtest_glove)\n",
    "    pred_train = np.zeros([xtrain_glove.shape[0], 5])\n",
    "    \n",
    "    for dev_index, val_index in kf.split(xtrain_glove):\n",
    "        dev_X, val_X = xtrain_glove[dev_index], xtrain_glove[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN_glove()\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=10, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_glove)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN_glove(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "# Fast Text\n",
    "\n",
    "def doAddFastText(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"ff_0\"] = pred_train[:,0]\n",
    "    X_train[\"ff_1\"] = pred_train[:,1]\n",
    "    X_train[\"ff_2\"] = pred_train[:,2]\n",
    "    X_train[\"ff_3\"] = pred_train[:,3]\n",
    "    X_train[\"ff_4\"] = pred_train[:,4]\n",
    "    X_test[\"ff_0\"] = pred_test[:,0]\n",
    "    X_test[\"ff_1\"] = pred_test[:,1]\n",
    "    X_test[\"ff_2\"] = pred_test[:,2]\n",
    "    X_test[\"ff_3\"] = pred_test[:,3]\n",
    "    X_test[\"ff_4\"] = pred_test[:,4]\n",
    "    return X_train,X_test\n",
    "\n",
    "\n",
    "def initFastText(embedding_dims,input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def preprocessFastText(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text\n",
    "\n",
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocessFastText(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def doFastText(X_train,X_test,Y_train):\n",
    "    min_count = 2\n",
    "\n",
    "    docs = create_docs(X_train)\n",
    "    tokenizer = Tokenizer(lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "    maxlen = 300\n",
    "\n",
    "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "    input_dim = np.max(docs) + 1\n",
    "    embedding_dims = 20\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "\n",
    "    docs_test = create_docs(X_test)\n",
    "    docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "    xtrain_pad = docs\n",
    "    xtest_pad = docs_test\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 5])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initFastText(embedding_dims,input_dim)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=25, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(docs_test)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddFastText(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "X_train,X_test = doFastText(X_train,X_test,Y_train)\n",
    "X_train,X_test = doNN(X_train,X_test,Y_train)\n",
    "X_train,X_test = doNN_glove(X_train,X_test,Y_train,glove_vecs_train,glove_vecs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:24:06] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.44020\ttest-mlogloss:1.44360\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.48894\ttest-mlogloss:0.51624\n",
      "[40]\ttrain-mlogloss:0.36624\ttest-mlogloss:0.41435\n",
      "[60]\ttrain-mlogloss:0.32861\ttest-mlogloss:0.39657\n",
      "[80]\ttrain-mlogloss:0.30391\ttest-mlogloss:0.39082\n",
      "[100]\ttrain-mlogloss:0.28196\ttest-mlogloss:0.38816\n",
      "[120]\ttrain-mlogloss:0.26241\ttest-mlogloss:0.38666\n",
      "[140]\ttrain-mlogloss:0.24459\ttest-mlogloss:0.38565\n",
      "[160]\ttrain-mlogloss:0.22787\ttest-mlogloss:0.38532\n",
      "[180]\ttrain-mlogloss:0.21313\ttest-mlogloss:0.38496\n",
      "[200]\ttrain-mlogloss:0.19929\ttest-mlogloss:0.38463\n",
      "[220]\ttrain-mlogloss:0.18695\ttest-mlogloss:0.38427\n",
      "[240]\ttrain-mlogloss:0.17503\ttest-mlogloss:0.38440\n",
      "[260]\ttrain-mlogloss:0.16414\ttest-mlogloss:0.38478\n",
      "Stopping. Best iteration:\n",
      "[228]\ttrain-mlogloss:0.18188\ttest-mlogloss:0.38404\n",
      "\n",
      "[00:29:18] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.44004\ttest-mlogloss:1.44267\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.48851\ttest-mlogloss:0.51891\n",
      "[40]\ttrain-mlogloss:0.36617\ttest-mlogloss:0.41725\n",
      "[60]\ttrain-mlogloss:0.32818\ttest-mlogloss:0.39824\n",
      "[80]\ttrain-mlogloss:0.30304\ttest-mlogloss:0.39229\n",
      "[100]\ttrain-mlogloss:0.28066\ttest-mlogloss:0.38966\n",
      "[120]\ttrain-mlogloss:0.26223\ttest-mlogloss:0.38744\n",
      "[140]\ttrain-mlogloss:0.24435\ttest-mlogloss:0.38639\n",
      "[160]\ttrain-mlogloss:0.22916\ttest-mlogloss:0.38669\n",
      "[180]\ttrain-mlogloss:0.21404\ttest-mlogloss:0.38673\n",
      "Stopping. Best iteration:\n",
      "[139]\ttrain-mlogloss:0.24533\ttest-mlogloss:0.38636\n",
      "\n",
      "[00:32:54] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.43997\ttest-mlogloss:1.44380\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.48779\ttest-mlogloss:0.52639\n",
      "[40]\ttrain-mlogloss:0.36552\ttest-mlogloss:0.42345\n",
      "[60]\ttrain-mlogloss:0.32673\ttest-mlogloss:0.40446\n",
      "[80]\ttrain-mlogloss:0.30165\ttest-mlogloss:0.39813\n",
      "[100]\ttrain-mlogloss:0.27992\ttest-mlogloss:0.39538\n",
      "[120]\ttrain-mlogloss:0.26065\ttest-mlogloss:0.39356\n",
      "[140]\ttrain-mlogloss:0.24323\ttest-mlogloss:0.39206\n",
      "[160]\ttrain-mlogloss:0.22762\ttest-mlogloss:0.39107\n",
      "[180]\ttrain-mlogloss:0.21305\ttest-mlogloss:0.39062\n",
      "[200]\ttrain-mlogloss:0.19932\ttest-mlogloss:0.39115\n",
      "[220]\ttrain-mlogloss:0.18635\ttest-mlogloss:0.39070\n",
      "Stopping. Best iteration:\n",
      "[177]\ttrain-mlogloss:0.21532\ttest-mlogloss:0.39058\n",
      "\n",
      "[00:37:15] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.44029\ttest-mlogloss:1.44162\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.49080\ttest-mlogloss:0.51213\n",
      "[40]\ttrain-mlogloss:0.36795\ttest-mlogloss:0.40964\n",
      "[60]\ttrain-mlogloss:0.32899\ttest-mlogloss:0.39188\n",
      "[80]\ttrain-mlogloss:0.30420\ttest-mlogloss:0.38713\n",
      "[100]\ttrain-mlogloss:0.28302\ttest-mlogloss:0.38480\n",
      "[120]\ttrain-mlogloss:0.26279\ttest-mlogloss:0.38349\n",
      "[140]\ttrain-mlogloss:0.24477\ttest-mlogloss:0.38287\n",
      "[160]\ttrain-mlogloss:0.22864\ttest-mlogloss:0.38260\n",
      "[180]\ttrain-mlogloss:0.21405\ttest-mlogloss:0.38337\n",
      "[200]\ttrain-mlogloss:0.20023\ttest-mlogloss:0.38323\n",
      "Stopping. Best iteration:\n",
      "[156]\ttrain-mlogloss:0.23157\ttest-mlogloss:0.38234\n",
      "\n",
      "[00:41:10] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.43363\ttest-mlogloss:1.43291\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.48798\ttest-mlogloss:0.50809\n",
      "[40]\ttrain-mlogloss:0.36745\ttest-mlogloss:0.41105\n",
      "[60]\ttrain-mlogloss:0.32911\ttest-mlogloss:0.39428\n",
      "[80]\ttrain-mlogloss:0.30331\ttest-mlogloss:0.38866\n",
      "[100]\ttrain-mlogloss:0.28178\ttest-mlogloss:0.38564\n",
      "[120]\ttrain-mlogloss:0.26245\ttest-mlogloss:0.38446\n",
      "[140]\ttrain-mlogloss:0.24533\ttest-mlogloss:0.38359\n",
      "[160]\ttrain-mlogloss:0.22926\ttest-mlogloss:0.38299\n",
      "[180]\ttrain-mlogloss:0.21371\ttest-mlogloss:0.38265\n",
      "[200]\ttrain-mlogloss:0.20006\ttest-mlogloss:0.38254\n",
      "[220]\ttrain-mlogloss:0.18658\ttest-mlogloss:0.38315\n",
      "[240]\ttrain-mlogloss:0.17514\ttest-mlogloss:0.38368\n",
      "Stopping. Best iteration:\n",
      "[193]\ttrain-mlogloss:0.20506\ttest-mlogloss:0.38251\n",
      "\n",
      "cv scores :  [0.38404312992420286, 0.3863583009575495, 0.39057757107939883, 0.3823430674453724, 0.38250879232159984]\n"
     ]
    }
   ],
   "source": [
    "# Final Model\n",
    "# XGBoost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 5\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 5\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    drop_columns=[\"text\",\"words\"]\n",
    "    x_train = X_train.drop(drop_columns+['author'],axis=1)\n",
    "    x_test = X_test.drop(drop_columns,axis=1)\n",
    "    y_train = Y_train\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([x_train.shape[0], 5])\n",
    "    for dev_index, val_index in kf.split(x_train):\n",
    "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    return pred_full_test/5\n",
    "result = do(X_train,X_test,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38721132155730303\n",
      "0.38516617234562467\n"
     ]
    }
   ],
   "source": [
    "# cv scores :  [0.37971865148208395, 0.3913440671935568, 0.39145263387349694, 0.38940780721717677, 0.38413344802020066]\n",
    "# cv scores :  [0.38404312992420286, 0.3863583009575495, 0.39057757107939883, 0.3823430674453724, 0.38250879232159984]\n",
    "a =  [0.37971865148208395, 0.3913440671935568, 0.39145263387349694, 0.38940780721717677, 0.38413344802020066]\n",
    "b = [0.38404312992420286, 0.3863583009575495, 0.39057757107939883, 0.3823430674453724, 0.38250879232159984]\n",
    "print(sum(a)/5)\n",
    "print(sum(b)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.949521</td>\n",
       "      <td>0.032718</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.008775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>0.986013</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.007095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.998662</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.032035</td>\n",
       "      <td>0.963130</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.002147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.983971</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.005105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19612</th>\n",
       "      <td>19612</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.996147</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19613</th>\n",
       "      <td>19613</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.996871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19614</th>\n",
       "      <td>19614</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.998709</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19615</th>\n",
       "      <td>19615</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.998773</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19616</th>\n",
       "      <td>19616</td>\n",
       "      <td>0.982200</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.015261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19617 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index         0         1         2         3         4\n",
       "0          0  0.001581  0.949521  0.032718  0.007405  0.008775\n",
       "1          1  0.003760  0.986013  0.001274  0.001857  0.007095\n",
       "2          2  0.998662  0.000541  0.000154  0.000142  0.000501\n",
       "3          3  0.002060  0.032035  0.963130  0.000627  0.002147\n",
       "4          4  0.983971  0.004545  0.002014  0.004365  0.005105\n",
       "...      ...       ...       ...       ...       ...       ...\n",
       "19612  19612  0.002113  0.996147  0.000868  0.000574  0.000297\n",
       "19613  19613  0.001742  0.000617  0.000579  0.000191  0.996871\n",
       "19614  19614  0.000516  0.998709  0.000332  0.000246  0.000198\n",
       "19615  19615  0.000287  0.998773  0.000489  0.000334  0.000117\n",
       "19616  19616  0.982200  0.000623  0.001428  0.000488  0.015261\n",
       "\n",
       "[19617 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission=pd.read_csv('open/sample_submission.csv', encoding='utf-8')\n",
    "sample_submission[['0', '1', '2', '3', '4']] = result\n",
    "sample_submission.to_csv(\"kg_3_1122.csv\", index=False)\n",
    "sample_submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
