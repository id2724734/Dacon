{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/11/26 21:14\n",
      "2020/11/26 21:14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "import time\n",
    " \n",
    "now = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min))\n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/11/26 21:14\n",
      "2020/11/26 21:14\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('open/train.csv')\n",
    "test = pd.read_csv('open/test_x.csv')\n",
    "submiss=pd.read_csv(\"open/sample_submission.csv\",index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train=train['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "y_train=train['author']\n",
    "X_Test=test['text'].str.replace('[^a-zA-Z0-9]', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_Train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    8.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    8.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.031\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.61324808532512\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2)],\n",
    "              'vect__max_df': [0.2, 0.25],\n",
    "              'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "              'clf__alpha': [0.031],\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    8.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    8.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.6087135875789627\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', CalibratedClassifierCV(MultinomialNB(alpha=0.05), method='isotonic')),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2)],\n",
    "              'vect__max_df': (0.4, 0.5),\n",
    "#               'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "#               'clf__alpha': (0.016, 0.019),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    7.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    7.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__min_df: 1\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.643483366095326\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', CalibratedClassifierCV(BernoulliNB(alpha=0.02), method='isotonic')),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2)],\n",
    "              'vect__max_df': [0.03],\n",
    "              'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "#               'clf__alpha': (0.016, 0.019),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   39.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   39.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.6120074265496387\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=1e-4, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "#               'vect__max_df': (0.5, 0.6),\n",
    "#               'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "#               'clf__alpha': (0.016, 0.019),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  4.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  4.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.5759024441009035\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "#               'vect__max_df': (0.5, 0.6),\n",
    "#               'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "#               'clf__alpha': (0.016, 0.019),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_iter=200 -> 0.5817073749174595   \n",
    "\n",
    "\n",
    "# C=10 -> 0.6128354782666344\n",
    "# C=20 -> 0.5903689734636244\n",
    "# C=30 -> 0.5817073749174595\n",
    "# C=40 -> 0.5781535999295956\n",
    "# C=45 -> 0.5793662895008217\n",
    "\n",
    "# C=50 -> 0.5759024441009035\n",
    "# C=60 -> 0.5789007294252998\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   29.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   29.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.5908551753092789\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB(alpha=0.03)),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "#               'vect__max_df': (0.5, 0.6),\n",
    "#               'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "#               'clf__alpha': (0.016, 0.019),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   31.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   31.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.572322278123051\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', CalibratedClassifierCV(MultinomialNB(alpha=0.03), method='isotonic')),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "#               'vect__max_df': (0.5, 0.6),\n",
    "#               'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "#               'clf__alpha': (0.016, 0.019),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   35.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   35.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.6481480500928002\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', CalibratedClassifierCV(BernoulliNB(alpha=0.03), method='isotonic')),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "#               'vect__max_df': (0.5, 0.6),\n",
    "#               'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "#               'clf__alpha': (0.016, 0.019),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   39.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   39.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.6447651217453803\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=1e-4, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "#               'vect__max_df': (0.5, 0.6),\n",
    "#               'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "#               'clf__alpha': (0.016, 0.019),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  6.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  6.7min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "0.5996524690884899\n"
     ]
    }
   ],
   "source": [
    "## Multinomial Naive Bayes Classifier ##\n",
    "# Build pipeline\n",
    "classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', LogisticRegression(C=30, max_iter = 200)),\n",
    "])\n",
    "\n",
    "# parameter tuning with grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 2), (1, 3)],\n",
    "#               'vect__max_df': (0.5, 0.6),\n",
    "#               'vect__min_df': [1],\n",
    "              'vect__analyzer' : ['word'],\n",
    "#               'clf__alpha': (0.016, 0.019),\n",
    "}\n",
    "gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "gs_clf.fit(x_train, y_train)\n",
    "\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_proba = gs_clf.predict_proba(x_test)\n",
    "print(log_loss(y_test, y_pred_proba))\n",
    "# y_pred_proba = gs_clf.predict_proba(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.6507361092389561\n",
    "# 0.5996524690884899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# Final Model\n",
    "# XGBoost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 5\n",
    "#     param['silent'] = 1\n",
    "    param['num_class'] = 5\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(train, test, Y_train):\n",
    "    drop_columns=['index', \"text\"]\n",
    "    x_train = train.drop(drop_columns+['author'],axis=1)\n",
    "    x_test = test.drop(drop_columns,axis=1)\n",
    "    y_train = Y_train\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=32143233)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([x_train.shape[0], 5])\n",
    "    for dev_index, val_index in kf.split(x_train):\n",
    "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv score : \", cv_scores)\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    return pred_full_test/5\n",
    "result = do(train, test, Y_train)\n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submiss['0']=y_pred_proba[:,0]\n",
    "submiss['1']=y_pred_proba[:,1]\n",
    "submiss['2']=y_pred_proba[:,2]\n",
    "submiss['3']=y_pred_proba[:,3]\n",
    "submiss['4']=y_pred_proba[:,4]\n",
    "submiss.to_csv('practice.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
