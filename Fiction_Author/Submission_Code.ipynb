{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Dacon] 소설 작가 분류 AI 경진대회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import metrics, preprocessing, pipeline, model_selection, naive_bayes\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import time\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling1D, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 200)\n",
    "train = pd.read_csv('open/train.csv')\n",
    "test = pd.read_csv('open/test_x.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "Y_train = LabelEncoder().fit_transform(train['author'])\n",
    "y_train=train['author']\n",
    "X_test=test['text'].str.replace('[^a-zA-Z0-9]', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구두점 비율(문장 안에 각 부호가 얼마나 있는지)\n",
    "punctuations = [{\"id\":1, \"p\" : \"[;:]\"},\n",
    "                {\"id\":2, \"p\" : \"[,.]\"},\n",
    "                {\"id\":3, \"p\" : \"[?]\"},\n",
    "                {\"id\":4, \"p\" : \"[!]\"},\n",
    "                {\"id\":5, \"p\" : \"[‘’\\']\"},\n",
    "                {\"id\":6, \"p\" : \"[“”\\\"]\"},\n",
    "                {\"id\":7, \"p\" : \"[;:,.?!\\'“”‘’\\\"]\"}]\n",
    "\n",
    "for p in punctuations:\n",
    "    punctuation = p[\"p\"]\n",
    "    _train =  [sentence.split() for sentence in train['text']]\n",
    "    train['punc_' + str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _train]\n",
    "\n",
    "    _test =  [sentence.split() for sentence in test['text']]\n",
    "    test['punc_' + str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))]) * 100 / len(sentence) for sentence in _test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "   - TfidfVectorizer\n",
    "   - CountVectorizer\n",
    "           analyzer : word, char, char_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer - word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 20:40\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   21.6s remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   23.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.031\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.3\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   18.9s remaining:   57.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   19.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.031\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   16.5s remaining:   49.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   18.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.031\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   19.3s remaining:   58.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   20.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.031\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.3\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   18.6s remaining:   55.9s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   19.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.031\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.6029955683225479, 0.604635802543013, 0.6211772682884821, 0.6004100043878932, 0.6154952568908586]\n",
      "Mean cv score :  0.608942780086559\n",
      "2020/12/10 20:40\n",
      "2020/12/10 20:44\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "                  'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "                  'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_MNB_0\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_MNB_1\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_MNB_2\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_MNB_3\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_MNB_4\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_MNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_MNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_MNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_MNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_MNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 20:44\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.4\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.5976665473926792, 0.5975338255728783, 0.6173897988183089, 0.5963371295574684, 0.6096579691870107]\n",
      "Mean cv score :  0.6037170541056691\n",
      "2020/12/10 20:44\n",
      "2020/12/10 20:48\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "                  'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CMNB_0\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CMNB_1\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CMNB_2\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CMNB_3\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CMNB_4\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CMNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CMNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CMNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CMNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CMNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 20:48\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   18.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   18.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   19.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   19.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   18.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   18.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.6292165710596017, 0.6394984741232733, 0.646819786619428, 0.6312645279907801, 0.6562403089836737]\n",
      "Mean cv score :  0.6406079337553514\n",
      "2020/12/10 20:48\n",
      "2020/12/10 20:52\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "                  'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CBNB_0\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CBNB_1\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CBNB_2\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CBNB_3\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CBNB_4\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CBNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CBNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CBNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CBNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CBNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 20:52\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   21.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   21.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   20.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   20.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   21.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   21.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   20.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   20.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.6034359823604672, 0.6184589677348814, 0.6177396133835682, 0.6024721411834132, 0.6200243325185175]\n",
      "Mean cv score :  0.6124262074361695\n",
      "2020/12/10 20:52\n",
      "2020/12/10 20:57\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CH_0\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CH_1\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CH_2\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CH_3\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CH_4\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CH_0\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CH_1\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CH_2\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CH_3\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CH_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 20:57\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.4min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.5620624641494352, 0.5857273048883577, 0.5885432314650478, 0.5693716851561413, 0.5916510341517979]\n",
      "Mean cv score :  0.579471143962156\n",
      "2020/12/10 20:57\n",
      "2020/12/10 21:26\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_L_0\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_L_1\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_L_2\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_L_3\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_L_4\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_L_0\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_L_1\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_L_2\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_L_3\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_L_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer - word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 21:26\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   15.9s remaining:   47.9s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   20.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.024\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   18.0s remaining:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   18.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.024\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   17.6s remaining:   52.9s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   18.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.031\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   18.3s remaining:   55.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   19.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.024\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   17.7s remaining:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:   19.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__alpha: 0.024\n",
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.25\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.5776704326928397, 0.5804705854660563, 0.6017163249986536, 0.5748083523087922, 0.5919857145779784]\n",
      "Mean cv score :  0.5853302820088641\n",
      "2020/12/10 21:26\n",
      "2020/12/10 21:30\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "                  'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "                  'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_MNB_0\"] = pred_train[ : , 0]\n",
    "train[\"count_MNB_1\"] = pred_train[ : , 1]\n",
    "train[\"count_MNB_2\"] = pred_train[ : , 2]\n",
    "train[\"count_MNB_3\"] = pred_train[ : , 3]\n",
    "train[\"count_MNB_4\"] = pred_train[ : , 4]\n",
    "test[\"count_MNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_MNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_MNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_MNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_MNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 21:30\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   14.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.4\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   17.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.4\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.4\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.4\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.4\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.5721619114226169, 0.5751870347960057, 0.5952297376162479, 0.5675133432744751, 0.5877951662222627]\n",
      "Mean cv score :  0.5795774386663217\n",
      "2020/12/10 21:30\n",
      "2020/12/10 21:33\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "                  'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CMNB_0\"] = pred_train[ : , 0]\n",
    "train[\"count_CMNB_1\"] = pred_train[ : , 1]\n",
    "train[\"count_CMNB_2\"] = pred_train[ : , 2]\n",
    "train[\"count_CMNB_3\"] = pred_train[ : , 3]\n",
    "train[\"count_CMNB_4\"] = pred_train[ : , 4]\n",
    "test[\"count_CMNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CMNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CMNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CMNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CMNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 21:33\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   16.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   18.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   18.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 102.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 102.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__max_df: 0.03\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.6292165710596003, 0.6394984741232734, 0.6468197866192531, 0.6312645279907797, 0.6562403089836758]\n",
      "Mean cv score :  0.6406079337553165\n",
      "2020/12/10 21:33\n",
      "2020/12/10 23:18\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "                  'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CBNB_0\"] = pred_train[ : , 0]\n",
    "train[\"count_CBNB_1\"] = pred_train[ : , 1]\n",
    "train[\"count_CBNB_2\"] = pred_train[ : , 2]\n",
    "train[\"count_CBNB_3\"] = pred_train[ : , 3]\n",
    "train[\"count_CBNB_4\"] = pred_train[ : , 4]\n",
    "test[\"count_CBNB_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CBNB_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CBNB_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CBNB_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CBNB_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 23:18\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   10.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   10.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   11.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   11.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    9.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    9.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    9.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    9.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    9.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    9.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.6112053600471076, 0.6304643525606525, 0.6301438865457136, 0.6108571808403607, 0.6309156687101501]\n",
      "Mean cv score :  0.622717289740797\n",
      "2020/12/10 23:18\n",
      "2020/12/10 23:21\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CH_0\"] = pred_train[ : , 0]\n",
    "train[\"count_CH_1\"] = pred_train[ : , 1]\n",
    "train[\"count_CH_2\"] = pred_train[ : , 2]\n",
    "train[\"count_CH_3\"] = pred_train[ : , 3]\n",
    "train[\"count_CH_4\"] = pred_train[ : , 4]\n",
    "test[\"count_CH_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CH_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CH_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CH_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CH_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 23:21\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'word'\n",
      "\tvect__ngram_range: (1, 2)\n",
      "cv score :  [0.59456263122513, 0.6247646427582598, 0.6199121419343707, 0.5983970572232508, 0.6207081495507819]\n",
      "Mean cv score :  0.6116689245383586\n",
      "2020/12/10 23:21\n",
      "2020/12/10 23:40\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 2)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['word'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_L_0\"] = pred_train[ : , 0]\n",
    "train[\"count_L_1\"] = pred_train[ : , 1]\n",
    "train[\"count_L_2\"] = pred_train[ : , 2]\n",
    "train[\"count_L_3\"] = pred_train[ : , 3]\n",
    "train[\"count_L_4\"] = pred_train[ : , 4]\n",
    "test[\"count_L_0\"] = pred_full_test[ : , 0]\n",
    "test[\"count_L_1\"] = pred_full_test[ : , 1]\n",
    "test[\"count_L_2\"] = pred_full_test[ : , 2]\n",
    "test[\"count_L_3\"] = pred_full_test[ : , 3]\n",
    "test[\"count_L_4\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer - char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 23:40\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   21.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   19.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   19.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   17.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   17.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   13.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   13.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "cv score :  [1.0115280239076807, 1.0125400559034543, 1.0284658391939439, 1.012186660775046, 1.032666145731224]\n",
      "Mean cv score :  1.01947734510227\n",
      "2020/12/10 23:40\n",
      "2020/12/10 23:44\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 3)],\n",
    "#                   'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_MNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_MNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_MNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_MNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_MNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_MNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_MNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_MNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_MNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_MNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/10 23:44\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  3.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  3.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  3.4min finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  3.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "cv score :  [0.583538896506643, 0.5725063199060331, 0.5955298726762939, 0.5699324625848107, 0.6007632148029367]\n",
      "Mean cv score :  0.5844541532953433\n",
      "2020/12/10 23:44\n",
      "2020/12/11 00:16\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6), (1, 7)],\n",
    "#                   'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CMNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CMNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CMNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CMNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CMNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CMNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CMNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CMNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CMNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CMNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 00:16\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score :  [0.775734358442237, 0.78177261076148, 0.813803751371879, 0.7902246807326629, 0.8351873041845946]\n",
      "Mean cv score :  0.7993445410985707\n",
      "2020/12/11 00:16\n",
      "2020/12/11 00:42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CBNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CBNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CBNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CBNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CBNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CBNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CBNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CBNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CBNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CBNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 00:42\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "cv score :  [0.5902080924675737, 0.5987877920592308, 0.6051677872017293, 0.585456933590771, 0.5980923984285439]\n",
      "Mean cv score :  0.5955426007495698\n",
      "2020/12/11 00:42\n",
      "2020/12/11 01:13\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CH_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CH_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CH_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CH_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CH_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CH_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CH_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CH_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CH_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CH_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 01:13\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.2min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.1min finished\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.1min remaining:    0.0s\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.1min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.1min finished\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.1min remaining:    0.0s\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.1min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "cv score :  [0.550177760163995, 0.5637414305437569, 0.5709449401238513, 0.5457623095916745, 0.5677299802712953]\n",
      "Mean cv score :  0.5596712841389146\n",
      "2020/12/11 01:13\n",
      "2020/12/11 03:12\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_L_0_char\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_L_1_char\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_L_2_char\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_L_3_char\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_L_4_char\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_L_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_L_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_L_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_L_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_L_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer - char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 03:12\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   20.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   20.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   19.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   19.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   20.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   20.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   19.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   19.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   18.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 3)\n",
      "cv score :  [1.2025769972789655, 1.196017341161219, 1.2158048511371249, 1.204632802204555, 1.215416507101536]\n",
      "Mean cv score :  1.20688969977668\n",
      "2020/12/11 03:12\n",
      "2020/12/11 03:16\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 3)],\n",
    "#                   'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_MNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_MNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_MNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_MNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_MNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_MNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_MNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_MNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_MNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_MNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 03:16\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "cv score :  [0.6062053488691869, 0.608204380895722, 0.6301709027666109, 0.6004675728066964, 0.6262398088058964]\n",
      "Mean cv score :  0.6142576028288225\n",
      "2020/12/11 03:16\n",
      "2020/12/11 03:38\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6), (1, 7)],\n",
    "#                   'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CMNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_CMNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_CMNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_CMNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_CMNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_CMNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CMNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CMNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CMNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CMNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 03:38\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  3.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score :  [0.7757343584422367, 0.7817726107614854, 0.813803751371872, 0.7902246807326817, 0.8351873041845956]\n",
      "Mean cv score :  0.7993445410985742\n",
      "2020/12/11 03:38\n",
      "2020/12/11 04:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CBNB_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_CBNB_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_CBNB_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_CBNB_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_CBNB_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_CBNB_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CBNB_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CBNB_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CBNB_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CBNB_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 04:04\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "cv score :  [0.6035258738327683, 0.619852610467435, 0.623098286127428, 0.6024450777603059, 0.6164943895087571]\n",
      "Mean cv score :  0.6130832475393388\n",
      "2020/12/11 04:04\n",
      "2020/12/11 04:34\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CH_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_CH_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_CH_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_CH_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_CH_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_CH_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CH_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CH_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CH_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CH_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 04:34\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.8min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.7min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.5min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "cv score :  [0.5886968390280568, 0.6193664081052572, 0.6113992355748873, 0.5914337779896759, 0.6181004671659751]\n",
      "Mean cv score :  0.6057993455727704\n",
      "2020/12/11 04:34\n",
      "2020/12/11 06:41\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_L_0_char\"] = pred_train[ : , 0]\n",
    "train[\"count_L_1_char\"] = pred_train[ : , 1]\n",
    "train[\"count_L_2_char\"] = pred_train[ : , 2]\n",
    "train[\"count_L_3_char\"] = pred_train[ : , 3]\n",
    "train[\"count_L_4_char\"] = pred_train[ : , 4]\n",
    "test[\"count_L_0_char\"] = pred_full_test[ : , 0]\n",
    "test[\"count_L_1_char\"] = pred_full_test[ : , 1]\n",
    "test[\"count_L_2_char\"] = pred_full_test[ : , 2]\n",
    "test[\"count_L_3_char\"] = pred_full_test[ : , 3]\n",
    "test[\"count_L_4_char\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer - char_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 06:41\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   32.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   32.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   31.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   31.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   30.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   30.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   31.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   31.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   28.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   28.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "cv score :  [0.9033411197664454, 0.892041555707428, 0.9261146817577017, 0.8996159895761203, 0.9281333671317467]\n",
      "Mean cv score :  0.9098493427878885\n",
      "2020/12/11 06:41\n",
      "2020/12/11 06:48\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 4)],\n",
    "#                   'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_MNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_MNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_MNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_MNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_MNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_MNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_MNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_MNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_MNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_MNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 06:48\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "cv score :  [0.6937506845908086, 0.7037523543429575, 0.7205482551194616, 0.6907853135222449, 0.7125186653595897]\n",
      "Mean cv score :  0.7042710545870124\n",
      "2020/12/11 06:48\n",
      "2020/12/11 07:00\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6), (1, 7)],\n",
    "#                   'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CMNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CMNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CMNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CMNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CMNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CMNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CMNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CMNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CMNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CMNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 07:00\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   44.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   44.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   47.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   47.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   47.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   47.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score :  [0.8701149893515928, 0.8763124922320249, 0.8968884931320447, 0.8804419941372349, 0.9200480018935118]\n",
      "Mean cv score :  0.8887611941492818\n",
      "2020/12/11 07:00\n",
      "2020/12/11 07:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CBNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CBNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CBNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CBNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CBNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CBNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CBNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CBNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CBNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CBNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 07:11\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "cv score :  [0.6817908383675271, 0.6904015298317142, 0.700992613589556, 0.6792673219766172, 0.7008159298128009]\n",
      "Mean cv score :  0.6906536467156431\n",
      "2020/12/11 07:11\n",
      "2020/12/11 07:31\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 5)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_CH_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_CH_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_CH_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_CH_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_CH_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_CH_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_CH_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_CH_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_CH_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_CH_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 07:31\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 5)\n",
      "cv score :  [0.7531322283836422, 0.7595884902535756, 0.7691192904545809, 0.7397663325723934, 0.7664605038631621]\n",
      "Mean cv score :  0.757613369105471\n",
      "2020/12/11 07:31\n",
      "2020/12/11 07:53\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# tfidf_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', TfidfVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 5)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"tfidf_L_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"tfidf_L_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"tfidf_L_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"tfidf_L_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"tfidf_L_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"tfidf_L_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"tfidf_L_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"tfidf_L_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"tfidf_L_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"tfidf_L_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer - char_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 07:53\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   32.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   32.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   28.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   28.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   30.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   30.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   29.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   29.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   30.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   30.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 4)\n",
      "cv score :  [1.1986562546767974, 1.175329222155078, 1.2152358670264753, 1.2004725140495378, 1.2106382239947202]\n",
      "Mean cv score :  1.2000664163805217\n",
      "2020/12/11 07:53\n",
      "2020/12/11 08:00\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_MNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 4)],\n",
    "#                   'vect__max_df': (0.25, 0.3),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': [0.024, 0.031],\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_MNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_MNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_MNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_MNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_MNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_MNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_MNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_MNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_MNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_MNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 08:00\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "cv score :  [0.708077687275612, 0.7115430477051486, 0.7358107130375974, 0.7060687560408367, 0.7375644805273464]\n",
      "Mean cv score :  0.7198129369173082\n",
      "2020/12/11 08:00\n",
      "2020/12/11 08:12\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CMNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(MultinomialNB(alpha = 0.05), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6), (1, 7)],\n",
    "#                   'vect__max_df': (0.4, 0.5),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CMNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_CMNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_CMNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_CMNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_CMNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_CMNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CMNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CMNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CMNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CMNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 08:12\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   46.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   46.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   46.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   46.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:   45.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv score :  [0.870114989351585, 0.8763124922320114, 0.8968884931320346, 0.8804419941372403, 0.9200480018935181]\n",
      "Mean cv score :  0.8887611941492779\n",
      "2020/12/11 08:12\n",
      "2020/12/11 08:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\calibration.py:381: RuntimeWarning: invalid value encountered in true_divide\n",
      "  proba /= np.sum(proba, axis=1)[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CBNB_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(BernoulliNB(alpha = 0.02), method='isotonic')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.03, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CBNB_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_CBNB_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_CBNB_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_CBNB_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_CBNB_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_CBNB_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CBNB_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CBNB_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CBNB_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CBNB_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 08:23\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 6)\n",
      "cv score :  [0.6656350773335611, 0.6802320699742936, 0.690574878949303, 0.6684665117459598, 0.6870881450584853]\n",
      "Mean cv score :  0.6783993366123207\n",
      "2020/12/11 08:23\n",
      "2020/12/11 08:46\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_CH_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', CalibratedClassifierCV(SGDClassifier(loss='modified_huber', alpha=0.00001, max_iter=10000, tol=1e-4), method='sigmoid')),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 6)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_CH_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_CH_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_CH_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_CH_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_CH_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_CH_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_CH_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_CH_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_CH_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_CH_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 08:46\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.7min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.6min finished\n",
      "C:\\Users\\id272\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvect__analyzer: 'char_wb'\n",
      "\tvect__ngram_range: (1, 7)\n",
      "cv score :  [0.6918741310827037, 0.7141652313957813, 0.7106886108970111, 0.696486717150301, 0.7197985019280598]\n",
      "Mean cv score :  0.7066026384907713\n",
      "2020/12/11 08:46\n",
      "2020/12/11 09:20\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# count_L_\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits = 5, shuffle = True, random_state = 32143233)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = X_train[dev_index], X_train[val_index]\n",
    "    dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "\n",
    "    classifier = Pipeline([('vect', CountVectorizer(lowercase=False)),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('clf', LogisticRegression(C=50, max_iter=200)),\n",
    "    ])\n",
    "    parameters = {'vect__ngram_range': [(1, 7)],\n",
    "#                   'vect__max_df': (0.3, 0.4),\n",
    "#                   'vect__min_df': [1],\n",
    "                  'vect__analyzer' : ['char_wb'],\n",
    "#                   'clf__alpha': (0.016, 0.018),\n",
    "    }\n",
    "    gs_clf = GridSearchCV(classifier, parameters, n_jobs=-1, verbose=1, cv=2)\n",
    "    gs_clf.fit(dev_X, dev_y)\n",
    "    best_parameters = gs_clf.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    pred_test_y = gs_clf.predict_proba(val_X)\n",
    "    pred_test_y2 = gs_clf.predict_proba(X_test)\n",
    "    pred_full_test = pred_full_test + pred_test_y2\n",
    "    pred_train[val_index, : ] = pred_test_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_test_y))\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5\n",
    "\n",
    "train[\"count_L_0_char_wb\"] = pred_train[ : , 0]\n",
    "train[\"count_L_1_char_wb\"] = pred_train[ : , 1]\n",
    "train[\"count_L_2_char_wb\"] = pred_train[ : , 2]\n",
    "train[\"count_L_3_char_wb\"] = pred_train[ : , 3]\n",
    "train[\"count_L_4_char_wb\"] = pred_train[ : , 4]\n",
    "test[\"count_L_0_char_wb\"] = pred_full_test[ : , 0]\n",
    "test[\"count_L_1_char_wb\"] = pred_full_test[ : , 1]\n",
    "test[\"count_L_2_char_wb\"] = pred_full_test[ : , 2]\n",
    "test[\"count_L_3_char_wb\"] = pred_full_test[ : , 3]\n",
    "test[\"count_L_4_char_wb\"] = pred_full_test[ : , 4]    \n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessFastText(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(';:,.?!\\'“”‘’\\\"')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text\n",
    "\n",
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocessFastText(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs\n",
    "docs = create_docs(train)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= 2])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = max([max(len(l) for l in docs)])\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "docs_test = create_docs(test)\n",
    "docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "\n",
    "xtrain_pad = docs\n",
    "xtest_pad = docs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20\n",
    "\n",
    "def initFastText(embedding_dims,input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 09:21\n",
      "Epoch 1/40\n",
      "1372/1372 [==============================] - 87s 63ms/step - loss: 1.5552 - accuracy: 0.2799 - val_loss: 1.5256 - val_accuracy: 0.2903\n",
      "Epoch 2/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 1.4633 - accuracy: 0.4170 - val_loss: 1.3933 - val_accuracy: 0.4677\n",
      "Epoch 3/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 1.2985 - accuracy: 0.5271 - val_loss: 1.2271 - val_accuracy: 0.5540\n",
      "Epoch 4/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 1.1324 - accuracy: 0.6102 - val_loss: 1.0876 - val_accuracy: 0.6394\n",
      "Epoch 5/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.9973 - accuracy: 0.6674 - val_loss: 0.9797 - val_accuracy: 0.6516\n",
      "Epoch 6/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.8880 - accuracy: 0.7148 - val_loss: 0.8925 - val_accuracy: 0.7198\n",
      "Epoch 7/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.7984 - accuracy: 0.7466 - val_loss: 0.8221 - val_accuracy: 0.7384\n",
      "Epoch 8/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.7219 - accuracy: 0.7770 - val_loss: 0.7645 - val_accuracy: 0.7411\n",
      "Epoch 9/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.6558 - accuracy: 0.8007 - val_loss: 0.7223 - val_accuracy: 0.7639\n",
      "Epoch 10/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.5986 - accuracy: 0.8201 - val_loss: 0.6765 - val_accuracy: 0.7823\n",
      "Epoch 11/40\n",
      "1372/1372 [==============================] - 93s 67ms/step - loss: 0.5480 - accuracy: 0.8380 - val_loss: 0.6450 - val_accuracy: 0.7827\n",
      "Epoch 12/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.5032 - accuracy: 0.8513 - val_loss: 0.6087 - val_accuracy: 0.8049\n",
      "Epoch 13/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.4634 - accuracy: 0.8658 - val_loss: 0.5846 - val_accuracy: 0.8111\n",
      "Epoch 14/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.4273 - accuracy: 0.8781 - val_loss: 0.5689 - val_accuracy: 0.8112\n",
      "Epoch 15/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.3953 - accuracy: 0.8874 - val_loss: 0.5519 - val_accuracy: 0.8201\n",
      "Epoch 16/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3665 - accuracy: 0.8972 - val_loss: 0.5268 - val_accuracy: 0.8253\n",
      "Epoch 17/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3399 - accuracy: 0.9053 - val_loss: 0.5153 - val_accuracy: 0.8279\n",
      "Epoch 18/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3154 - accuracy: 0.9132 - val_loss: 0.5035 - val_accuracy: 0.8311\n",
      "Epoch 19/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2934 - accuracy: 0.9208 - val_loss: 0.4903 - val_accuracy: 0.8322\n",
      "Epoch 20/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2732 - accuracy: 0.9254 - val_loss: 0.4824 - val_accuracy: 0.8380\n",
      "Epoch 21/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2549 - accuracy: 0.9315 - val_loss: 0.4696 - val_accuracy: 0.8427\n",
      "Epoch 22/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2379 - accuracy: 0.9365 - val_loss: 0.4741 - val_accuracy: 0.8347\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 1.5562 - accuracy: 0.2809 - val_loss: 1.5193 - val_accuracy: 0.2993\n",
      "Epoch 2/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 1.4628 - accuracy: 0.4308 - val_loss: 1.3897 - val_accuracy: 0.5355\n",
      "Epoch 3/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.2967 - accuracy: 0.5712 - val_loss: 1.2259 - val_accuracy: 0.5957\n",
      "Epoch 4/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 1.1281 - accuracy: 0.6302 - val_loss: 1.0909 - val_accuracy: 0.6420\n",
      "Epoch 5/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.9917 - accuracy: 0.6773 - val_loss: 0.9871 - val_accuracy: 0.6581\n",
      "Epoch 6/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.8832 - accuracy: 0.7146 - val_loss: 0.9033 - val_accuracy: 0.7054\n",
      "Epoch 7/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.7932 - accuracy: 0.7493 - val_loss: 0.8403 - val_accuracy: 0.7215\n",
      "Epoch 8/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.7173 - accuracy: 0.7771 - val_loss: 0.7827 - val_accuracy: 0.7452\n",
      "Epoch 9/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.6521 - accuracy: 0.7988 - val_loss: 0.7377 - val_accuracy: 0.7589\n",
      "Epoch 10/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.5955 - accuracy: 0.8203 - val_loss: 0.6969 - val_accuracy: 0.7766\n",
      "Epoch 11/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.5460 - accuracy: 0.8375 - val_loss: 0.6632 - val_accuracy: 0.7779\n",
      "Epoch 12/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.5021 - accuracy: 0.8526 - val_loss: 0.6326 - val_accuracy: 0.7931\n",
      "Epoch 13/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.4628 - accuracy: 0.8656 - val_loss: 0.6109 - val_accuracy: 0.8008\n",
      "Epoch 14/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.4276 - accuracy: 0.8767 - val_loss: 0.5858 - val_accuracy: 0.8041\n",
      "Epoch 15/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.3958 - accuracy: 0.8873 - val_loss: 0.5664 - val_accuracy: 0.8080\n",
      "Epoch 16/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3671 - accuracy: 0.8960 - val_loss: 0.5514 - val_accuracy: 0.8130\n",
      "Epoch 17/40\n",
      "1372/1372 [==============================] - 91s 66ms/step - loss: 0.3407 - accuracy: 0.9047 - val_loss: 0.5389 - val_accuracy: 0.8151\n",
      "Epoch 18/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3169 - accuracy: 0.9121 - val_loss: 0.5221 - val_accuracy: 0.8192\n",
      "Epoch 19/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.2953 - accuracy: 0.9181 - val_loss: 0.5131 - val_accuracy: 0.8207\n",
      "Epoch 20/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2751 - accuracy: 0.9243 - val_loss: 0.5044 - val_accuracy: 0.8239\n",
      "Epoch 21/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.2565 - accuracy: 0.9310 - val_loss: 0.4951 - val_accuracy: 0.8293\n",
      "Epoch 22/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2398 - accuracy: 0.9360 - val_loss: 0.4854 - val_accuracy: 0.8326\n",
      "Epoch 23/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2242 - accuracy: 0.9417 - val_loss: 0.4812 - val_accuracy: 0.8292\n",
      "Epoch 24/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.2101 - accuracy: 0.9463 - val_loss: 0.4742 - val_accuracy: 0.8335\n",
      "Epoch 25/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.1967 - accuracy: 0.9507 - val_loss: 0.4685 - val_accuracy: 0.8360\n",
      "Epoch 26/40\n",
      "1372/1372 [==============================] - 87s 64ms/step - loss: 0.1847 - accuracy: 0.9525 - val_loss: 0.4692 - val_accuracy: 0.8347\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 1.5571 - accuracy: 0.2877 - val_loss: 1.5336 - val_accuracy: 0.3834\n",
      "Epoch 2/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 1.4712 - accuracy: 0.4265 - val_loss: 1.4106 - val_accuracy: 0.4708\n",
      "Epoch 3/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.3091 - accuracy: 0.5572 - val_loss: 1.2476 - val_accuracy: 0.5810\n",
      "Epoch 4/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 1.1370 - accuracy: 0.6231 - val_loss: 1.1091 - val_accuracy: 0.6085\n",
      "Epoch 5/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.9964 - accuracy: 0.6740 - val_loss: 1.0026 - val_accuracy: 0.6532\n",
      "Epoch 6/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.8855 - accuracy: 0.7141 - val_loss: 0.9171 - val_accuracy: 0.6855\n",
      "Epoch 7/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.7938 - accuracy: 0.7474 - val_loss: 0.8551 - val_accuracy: 0.6972\n",
      "Epoch 8/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.7165 - accuracy: 0.7763 - val_loss: 0.7958 - val_accuracy: 0.7300\n",
      "Epoch 9/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.6499 - accuracy: 0.7998 - val_loss: 0.7465 - val_accuracy: 0.7408\n",
      "Epoch 10/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.5927 - accuracy: 0.8194 - val_loss: 0.7046 - val_accuracy: 0.7652\n",
      "Epoch 11/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.5424 - accuracy: 0.8373 - val_loss: 0.6736 - val_accuracy: 0.7780\n",
      "Epoch 12/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.4980 - accuracy: 0.8524 - val_loss: 0.6408 - val_accuracy: 0.7808\n",
      "Epoch 13/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.4576 - accuracy: 0.8654 - val_loss: 0.6150 - val_accuracy: 0.7909\n",
      "Epoch 14/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.4225 - accuracy: 0.8769 - val_loss: 0.5931 - val_accuracy: 0.7961\n",
      "Epoch 15/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3900 - accuracy: 0.8880 - val_loss: 0.5733 - val_accuracy: 0.7999\n",
      "Epoch 16/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3617 - accuracy: 0.8968 - val_loss: 0.5549 - val_accuracy: 0.8062\n",
      "Epoch 17/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.3349 - accuracy: 0.9061 - val_loss: 0.5432 - val_accuracy: 0.8099\n",
      "Epoch 18/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3111 - accuracy: 0.9125 - val_loss: 0.5291 - val_accuracy: 0.8154\n",
      "Epoch 19/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2893 - accuracy: 0.9191 - val_loss: 0.5145 - val_accuracy: 0.8256\n",
      "Epoch 20/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2696 - accuracy: 0.9264 - val_loss: 0.5088 - val_accuracy: 0.8222\n",
      "Epoch 21/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.2518 - accuracy: 0.9306 - val_loss: 0.4952 - val_accuracy: 0.8304\n",
      "Epoch 22/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.2350 - accuracy: 0.9377 - val_loss: 0.4934 - val_accuracy: 0.8281\n",
      "Epoch 23/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.2195 - accuracy: 0.9417 - val_loss: 0.4824 - val_accuracy: 0.8328\n",
      "Epoch 24/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.2052 - accuracy: 0.9460 - val_loss: 0.4764 - val_accuracy: 0.8390\n",
      "Epoch 25/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.1921 - accuracy: 0.9498 - val_loss: 0.4839 - val_accuracy: 0.8314\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 1.5554 - accuracy: 0.2840 - val_loss: 1.5317 - val_accuracy: 0.3895\n",
      "Epoch 2/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 1.4759 - accuracy: 0.4070 - val_loss: 1.4110 - val_accuracy: 0.4821\n",
      "Epoch 3/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.3191 - accuracy: 0.5427 - val_loss: 1.2427 - val_accuracy: 0.5630\n",
      "Epoch 4/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.1478 - accuracy: 0.6159 - val_loss: 1.1001 - val_accuracy: 0.6143\n",
      "Epoch 5/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.0068 - accuracy: 0.6675 - val_loss: 0.9851 - val_accuracy: 0.6583\n",
      "Epoch 6/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.8945 - accuracy: 0.7109 - val_loss: 0.8957 - val_accuracy: 0.6986\n",
      "Epoch 7/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.8022 - accuracy: 0.7456 - val_loss: 0.8290 - val_accuracy: 0.7196\n",
      "Epoch 8/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.7244 - accuracy: 0.7742 - val_loss: 0.7685 - val_accuracy: 0.7418\n",
      "Epoch 9/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.6580 - accuracy: 0.7979 - val_loss: 0.7217 - val_accuracy: 0.7590\n",
      "Epoch 10/40\n",
      "1372/1372 [==============================] - 90s 66ms/step - loss: 0.6014 - accuracy: 0.8175 - val_loss: 0.6795 - val_accuracy: 0.7765\n",
      "Epoch 11/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.5515 - accuracy: 0.8349 - val_loss: 0.6521 - val_accuracy: 0.7904\n",
      "Epoch 12/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.5071 - accuracy: 0.8506 - val_loss: 0.6185 - val_accuracy: 0.7924\n",
      "Epoch 13/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.4675 - accuracy: 0.8634 - val_loss: 0.5932 - val_accuracy: 0.8039\n",
      "Epoch 14/40\n",
      "1372/1372 [==============================] - 91s 66ms/step - loss: 0.4322 - accuracy: 0.8745 - val_loss: 0.5729 - val_accuracy: 0.8069\n",
      "Epoch 15/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.4003 - accuracy: 0.8861 - val_loss: 0.5557 - val_accuracy: 0.8086\n",
      "Epoch 16/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3715 - accuracy: 0.8936 - val_loss: 0.5378 - val_accuracy: 0.8169\n",
      "Epoch 17/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3455 - accuracy: 0.9023 - val_loss: 0.5246 - val_accuracy: 0.8210\n",
      "Epoch 18/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3215 - accuracy: 0.9110 - val_loss: 0.5062 - val_accuracy: 0.8300\n",
      "Epoch 19/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2995 - accuracy: 0.9180 - val_loss: 0.4965 - val_accuracy: 0.8313\n",
      "Epoch 20/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2793 - accuracy: 0.9233 - val_loss: 0.4853 - val_accuracy: 0.8373\n",
      "Epoch 21/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.2610 - accuracy: 0.9291 - val_loss: 0.4754 - val_accuracy: 0.8396\n",
      "Epoch 22/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.2437 - accuracy: 0.9344 - val_loss: 0.4685 - val_accuracy: 0.8421\n",
      "Epoch 23/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.2282 - accuracy: 0.9400 - val_loss: 0.4672 - val_accuracy: 0.8415\n",
      "Epoch 24/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.2130 - accuracy: 0.9444 - val_loss: 0.4540 - val_accuracy: 0.8478\n",
      "Epoch 25/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.1997 - accuracy: 0.9491 - val_loss: 0.4471 - val_accuracy: 0.8501\n",
      "Epoch 26/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.1875 - accuracy: 0.9514 - val_loss: 0.4443 - val_accuracy: 0.8492\n",
      "Epoch 27/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.1758 - accuracy: 0.9560 - val_loss: 0.4485 - val_accuracy: 0.8487\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.5570 - accuracy: 0.2755 - val_loss: 1.5357 - val_accuracy: 0.3903\n",
      "Epoch 2/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.4790 - accuracy: 0.4043 - val_loss: 1.4228 - val_accuracy: 0.4424\n",
      "Epoch 3/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.3169 - accuracy: 0.5540 - val_loss: 1.2525 - val_accuracy: 0.5969\n",
      "Epoch 4/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.1417 - accuracy: 0.6261 - val_loss: 1.1053 - val_accuracy: 0.6203\n",
      "Epoch 5/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 1.0004 - accuracy: 0.6765 - val_loss: 0.9947 - val_accuracy: 0.6628\n",
      "Epoch 6/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.8889 - accuracy: 0.7140 - val_loss: 0.9093 - val_accuracy: 0.7097\n",
      "Epoch 7/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.7978 - accuracy: 0.7489 - val_loss: 0.8377 - val_accuracy: 0.7232\n",
      "Epoch 8/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.7208 - accuracy: 0.7767 - val_loss: 0.7806 - val_accuracy: 0.7468\n",
      "Epoch 9/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.6546 - accuracy: 0.7998 - val_loss: 0.7349 - val_accuracy: 0.7523\n",
      "Epoch 10/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.5975 - accuracy: 0.8183 - val_loss: 0.6949 - val_accuracy: 0.7743\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.5480 - accuracy: 0.8350 - val_loss: 0.6616 - val_accuracy: 0.7732\n",
      "Epoch 12/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.5028 - accuracy: 0.8505 - val_loss: 0.6319 - val_accuracy: 0.7944\n",
      "Epoch 13/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.4631 - accuracy: 0.8650 - val_loss: 0.6075 - val_accuracy: 0.7994\n",
      "Epoch 14/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.4275 - accuracy: 0.8762 - val_loss: 0.5820 - val_accuracy: 0.8068\n",
      "Epoch 15/40\n",
      "1372/1372 [==============================] - 91s 66ms/step - loss: 0.3951 - accuracy: 0.8870 - val_loss: 0.5682 - val_accuracy: 0.8059\n",
      "Epoch 16/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3660 - accuracy: 0.8954 - val_loss: 0.5558 - val_accuracy: 0.8157\n",
      "Epoch 17/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3397 - accuracy: 0.9054 - val_loss: 0.5308 - val_accuracy: 0.8218\n",
      "Epoch 18/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.3155 - accuracy: 0.9124 - val_loss: 0.5183 - val_accuracy: 0.8255\n",
      "Epoch 19/40\n",
      "1372/1372 [==============================] - 91s 66ms/step - loss: 0.2937 - accuracy: 0.9192 - val_loss: 0.5142 - val_accuracy: 0.8280\n",
      "Epoch 20/40\n",
      "1372/1372 [==============================] - 88s 64ms/step - loss: 0.2737 - accuracy: 0.9253 - val_loss: 0.5052 - val_accuracy: 0.8272\n",
      "Epoch 21/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.2551 - accuracy: 0.9316 - val_loss: 0.5044 - val_accuracy: 0.8237\n",
      "Epoch 22/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2383 - accuracy: 0.9359 - val_loss: 0.4879 - val_accuracy: 0.8330\n",
      "Epoch 23/40\n",
      "1372/1372 [==============================] - 90s 65ms/step - loss: 0.2224 - accuracy: 0.9420 - val_loss: 0.4780 - val_accuracy: 0.8354\n",
      "Epoch 24/40\n",
      "1372/1372 [==============================] - 89s 65ms/step - loss: 0.2081 - accuracy: 0.9447 - val_loss: 0.4797 - val_accuracy: 0.8395\n",
      "\n",
      "\n",
      "\n",
      "cv score :  [0.4740999388400997, 0.46920848253241726, 0.4839351044227665, 0.44849345774334637, 0.47972955347481894]\n",
      "Mean cv score :  0.47109330740268973\n",
      "2020/12/11 09:21\n",
      "2020/12/11 12:25\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([xtrain_pad.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=32143233)\n",
    "\n",
    "for dev_index, val_index in kf.split(xtrain_pad):\n",
    "    dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "    dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "    \n",
    "    model = initFastText(embedding_dims,input_dim)\n",
    "    model.fit(dev_X, dev_y,\n",
    "              batch_size=32, \n",
    "              epochs=40, \n",
    "              verbose=1, \n",
    "              validation_data=(val_X, val_y),\n",
    "              callbacks=[earlyStopping])\n",
    "    \n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(xtest_pad)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print('')\n",
    "    print('')    \n",
    "    print('')    \n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5 \n",
    "\n",
    "train[\"ff_0\"] = pred_train[:,0]\n",
    "train[\"ff_1\"] = pred_train[:,1]\n",
    "train[\"ff_2\"] = pred_train[:,2]\n",
    "train[\"ff_3\"] = pred_train[:,3]\n",
    "train[\"ff_4\"] = pred_train[:,4]\n",
    "test[\"ff_0\"] = pred_full_test[:,0]\n",
    "test[\"ff_1\"] = pred_full_test[:,1]\n",
    "test[\"ff_2\"] = pred_full_test[:,2]\n",
    "test[\"ff_3\"] = pred_full_test[:,3]\n",
    "test[\"ff_4\"] = pred_full_test[:,4]\n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 70\n",
    "nb_words = 10000\n",
    "\n",
    "texts_1 = []\n",
    "for text in train['text']:\n",
    "    texts_1.append(text)\n",
    "\n",
    "test_texts_1 = []\n",
    "for text in test['text']:\n",
    "    test_texts_1.append(text)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=nb_words)\n",
    "tokenizer.fit_on_texts(texts_1)\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "\n",
    "xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n",
    "xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n",
    "del test_sequences_1\n",
    "del sequences_1\n",
    "nb_words_cnt = min(nb_words, len(word_index)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initNN(nb_words_cnt, max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64, 5, padding='valid', activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 12:25\n",
      "Epoch 1/3\n",
      "1372/1372 [==============================] - 36s 26ms/step - loss: 1.0417 - accuracy: 0.5812 - val_loss: 0.7483 - val_accuracy: 0.7252\n",
      "Epoch 2/3\n",
      "1372/1372 [==============================] - 37s 27ms/step - loss: 0.6582 - accuracy: 0.7562 - val_loss: 0.6632 - val_accuracy: 0.7582\n",
      "Epoch 3/3\n",
      "1372/1372 [==============================] - 43s 31ms/step - loss: 0.5361 - accuracy: 0.8037 - val_loss: 0.6515 - val_accuracy: 0.7621\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "1372/1372 [==============================] - 33s 24ms/step - loss: 1.0502 - accuracy: 0.5765 - val_loss: 0.7315 - val_accuracy: 0.7328\n",
      "Epoch 2/3\n",
      "1372/1372 [==============================] - 41s 30ms/step - loss: 0.6538 - accuracy: 0.7569 - val_loss: 0.6545 - val_accuracy: 0.7615\n",
      "Epoch 3/3\n",
      "1372/1372 [==============================] - 37s 27ms/step - loss: 0.5338 - accuracy: 0.8029 - val_loss: 0.6403 - val_accuracy: 0.7649\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "1372/1372 [==============================] - 38s 27ms/step - loss: 1.0394 - accuracy: 0.5794 - val_loss: 0.7807 - val_accuracy: 0.7065\n",
      "Epoch 2/3\n",
      "1372/1372 [==============================] - 42s 31ms/step - loss: 0.6603 - accuracy: 0.7538 - val_loss: 0.6794 - val_accuracy: 0.7443\n",
      "Epoch 3/3\n",
      "1372/1372 [==============================] - 33s 24ms/step - loss: 0.5354 - accuracy: 0.8023 - val_loss: 0.6710 - val_accuracy: 0.7515\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "1372/1372 [==============================] - 42s 30ms/step - loss: 1.0447 - accuracy: 0.5789 - val_loss: 0.7455 - val_accuracy: 0.7218\n",
      "Epoch 2/3\n",
      "1372/1372 [==============================] - 37s 27ms/step - loss: 0.6679 - accuracy: 0.7553 - val_loss: 0.6513 - val_accuracy: 0.7603\n",
      "Epoch 3/3\n",
      "1372/1372 [==============================] - 35s 25ms/step - loss: 0.5402 - accuracy: 0.8013 - val_loss: 0.6374 - val_accuracy: 0.7638\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/3\n",
      "1372/1372 [==============================] - 40s 29ms/step - loss: 1.0402 - accuracy: 0.5804 - val_loss: 0.7739 - val_accuracy: 0.7069\n",
      "Epoch 2/3\n",
      "1372/1372 [==============================] - 32s 24ms/step - loss: 0.6603 - accuracy: 0.7559 - val_loss: 0.6830 - val_accuracy: 0.7492\n",
      "Epoch 3/3\n",
      "1372/1372 [==============================] - 40s 29ms/step - loss: 0.5346 - accuracy: 0.8027 - val_loss: 0.6744 - val_accuracy: 0.7525\n",
      "\n",
      "\n",
      "\n",
      "cv score :  [0.6514716043131098, 0.6403169269187412, 0.6709669889555318, 0.6373779642953998, 0.6743653679029192]\n",
      "Mean cv score :  0.6548997704771403\n",
      "2020/12/11 12:25\n",
      "2020/12/11 12:35\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([xtrain_pad.shape[0], 5])\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=32143233)\n",
    "\n",
    "for dev_index, val_index in kf.split(xtrain_pad):\n",
    "    dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "    dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "    \n",
    "    model = initNN(nb_words_cnt, max_len)\n",
    "    model.fit(dev_X, dev_y,\n",
    "              batch_size=32,\n",
    "              epochs=3,\n",
    "              verbose=1,\n",
    "              validation_data=(val_X, val_y),\n",
    "              callbacks=[earlyStopping])\n",
    "    \n",
    "    pred_val_y = model.predict(val_X)\n",
    "    pred_test_y = model.predict(xtest_pad)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "print(\"cv score : \", cv_scores)\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5 \n",
    "\n",
    "train[\"nn_0\"] = pred_train[:,0]\n",
    "train[\"nn_1\"] = pred_train[:,1]\n",
    "train[\"nn_2\"] = pred_train[:,2]\n",
    "train[\"nn_3\"] = pred_train[:,3]\n",
    "train[\"nn_4\"] = pred_train[:,4]\n",
    "\n",
    "test[\"nn_0\"] = pred_full_test[:,0]\n",
    "test[\"nn_1\"] = pred_full_test[:,1]\n",
    "test[\"nn_2\"] = pred_full_test[:,2]\n",
    "test[\"nn_3\"] = pred_full_test[:,3]\n",
    "test[\"nn_4\"] = pred_full_test[:,4]\n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/12/11 12:35\n",
      "[0]\ttrain-mlogloss:1.42714\ttest-mlogloss:1.42988\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.45890\ttest-mlogloss:0.48008\n",
      "[40]\ttrain-mlogloss:0.33915\ttest-mlogloss:0.37481\n",
      "[60]\ttrain-mlogloss:0.30332\ttest-mlogloss:0.35502\n",
      "[80]\ttrain-mlogloss:0.28099\ttest-mlogloss:0.34956\n",
      "[100]\ttrain-mlogloss:0.26147\ttest-mlogloss:0.34749\n",
      "[120]\ttrain-mlogloss:0.24328\ttest-mlogloss:0.34703\n",
      "[140]\ttrain-mlogloss:0.22628\ttest-mlogloss:0.34653\n",
      "[160]\ttrain-mlogloss:0.21135\ttest-mlogloss:0.34641\n",
      "[180]\ttrain-mlogloss:0.19691\ttest-mlogloss:0.34603\n",
      "[200]\ttrain-mlogloss:0.18366\ttest-mlogloss:0.34636\n",
      "[220]\ttrain-mlogloss:0.17220\ttest-mlogloss:0.34668\n",
      "Stopping. Best iteration:\n",
      "[187]\ttrain-mlogloss:0.19212\ttest-mlogloss:0.34597\n",
      "\n",
      "[0]\ttrain-mlogloss:1.42719\ttest-mlogloss:1.42844\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.45715\ttest-mlogloss:0.48342\n",
      "[40]\ttrain-mlogloss:0.33721\ttest-mlogloss:0.38300\n",
      "[60]\ttrain-mlogloss:0.30230\ttest-mlogloss:0.36526\n",
      "[80]\ttrain-mlogloss:0.28033\ttest-mlogloss:0.35992\n",
      "[100]\ttrain-mlogloss:0.26059\ttest-mlogloss:0.35752\n",
      "[120]\ttrain-mlogloss:0.24230\ttest-mlogloss:0.35642\n",
      "[140]\ttrain-mlogloss:0.22564\ttest-mlogloss:0.35609\n",
      "[160]\ttrain-mlogloss:0.21078\ttest-mlogloss:0.35619\n",
      "[180]\ttrain-mlogloss:0.19693\ttest-mlogloss:0.35728\n",
      "[200]\ttrain-mlogloss:0.18420\ttest-mlogloss:0.35729\n",
      "Stopping. Best iteration:\n",
      "[150]\ttrain-mlogloss:0.21852\ttest-mlogloss:0.35594\n",
      "\n",
      "[0]\ttrain-mlogloss:1.42670\ttest-mlogloss:1.42952\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.45454\ttest-mlogloss:0.49368\n",
      "[40]\ttrain-mlogloss:0.33424\ttest-mlogloss:0.39573\n",
      "[60]\ttrain-mlogloss:0.29930\ttest-mlogloss:0.37806\n",
      "[80]\ttrain-mlogloss:0.27677\ttest-mlogloss:0.37209\n",
      "[100]\ttrain-mlogloss:0.25822\ttest-mlogloss:0.37002\n",
      "[120]\ttrain-mlogloss:0.24052\ttest-mlogloss:0.36894\n",
      "[140]\ttrain-mlogloss:0.22481\ttest-mlogloss:0.36872\n",
      "[160]\ttrain-mlogloss:0.20956\ttest-mlogloss:0.36896\n",
      "[180]\ttrain-mlogloss:0.19641\ttest-mlogloss:0.36960\n",
      "Stopping. Best iteration:\n",
      "[145]\ttrain-mlogloss:0.22072\ttest-mlogloss:0.36856\n",
      "\n",
      "[0]\ttrain-mlogloss:1.42785\ttest-mlogloss:1.42768\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.45934\ttest-mlogloss:0.47483\n",
      "[40]\ttrain-mlogloss:0.34004\ttest-mlogloss:0.37139\n",
      "[60]\ttrain-mlogloss:0.30477\ttest-mlogloss:0.35200\n",
      "[80]\ttrain-mlogloss:0.28235\ttest-mlogloss:0.34619\n",
      "[100]\ttrain-mlogloss:0.26207\ttest-mlogloss:0.34344\n",
      "[120]\ttrain-mlogloss:0.24470\ttest-mlogloss:0.34165\n",
      "[140]\ttrain-mlogloss:0.22841\ttest-mlogloss:0.34070\n",
      "[160]\ttrain-mlogloss:0.21389\ttest-mlogloss:0.34060\n",
      "[180]\ttrain-mlogloss:0.19943\ttest-mlogloss:0.34088\n",
      "[200]\ttrain-mlogloss:0.18745\ttest-mlogloss:0.34123\n",
      "Stopping. Best iteration:\n",
      "[150]\ttrain-mlogloss:0.22095\ttest-mlogloss:0.34039\n",
      "\n",
      "[0]\ttrain-mlogloss:1.42933\ttest-mlogloss:1.43515\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.45583\ttest-mlogloss:0.49513\n",
      "[40]\ttrain-mlogloss:0.33576\ttest-mlogloss:0.39406\n",
      "[60]\ttrain-mlogloss:0.30104\ttest-mlogloss:0.37549\n",
      "[80]\ttrain-mlogloss:0.27885\ttest-mlogloss:0.36939\n",
      "[100]\ttrain-mlogloss:0.25966\ttest-mlogloss:0.36655\n",
      "[120]\ttrain-mlogloss:0.24080\ttest-mlogloss:0.36498\n",
      "[140]\ttrain-mlogloss:0.22467\ttest-mlogloss:0.36378\n",
      "[160]\ttrain-mlogloss:0.20988\ttest-mlogloss:0.36355\n",
      "[180]\ttrain-mlogloss:0.19651\ttest-mlogloss:0.36386\n",
      "[200]\ttrain-mlogloss:0.18357\ttest-mlogloss:0.36400\n",
      "Stopping. Best iteration:\n",
      "[151]\ttrain-mlogloss:0.21692\ttest-mlogloss:0.36317\n",
      "\n",
      "cv score :  [0.34597326081983454, 0.35593560444121564, 0.3685622723107674, 0.3403886800870347, 0.3631713345466958]\n",
      "Mean cv score :  0.3548062304411096\n",
      "2020/12/11 12:35\n",
      "2020/12/11 12:58\n"
     ]
    }
   ],
   "source": [
    "start = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "# Final Model\n",
    "# XGBoost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 5\n",
    "#     param['silent'] = 1\n",
    "    param['num_class'] = 5\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(train, test, Y_train):\n",
    "    drop_columns=['index', \"text\"]\n",
    "    x_train = train.drop(drop_columns+['author'],axis=1)\n",
    "    x_test = test.drop(drop_columns,axis=1)\n",
    "    y_train = Y_train\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=32143233)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([x_train.shape[0], 5])\n",
    "    for dev_index, val_index in kf.split(x_train):\n",
    "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv score : \", cv_scores)\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    return pred_full_test/5\n",
    "result = do(train, test, Y_train)\n",
    "\n",
    "end = time.localtime()\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (start.tm_year, start.tm_mon, start.tm_mday, start.tm_hour, start.tm_min))\n",
    "print(\"%04d/%02d/%02d %02d:%02d\" % (end.tm_year, end.tm_mon, end.tm_mday, end.tm_hour, end.tm_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 및 결언\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>0.527903</td>\n",
       "      <td>0.446004</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>0.002052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.988308</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.002856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999346</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.996193</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.995207</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19612</th>\n",
       "      <td>19612</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.999574</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19613</th>\n",
       "      <td>19613</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.998794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19614</th>\n",
       "      <td>19614</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19615</th>\n",
       "      <td>19615</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.999480</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19616</th>\n",
       "      <td>19616</td>\n",
       "      <td>0.990892</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.007804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19617 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index         0         1         2         3         4\n",
       "0          0  0.009786  0.527903  0.446004  0.014256  0.002052\n",
       "1          1  0.004383  0.988308  0.002804  0.001649  0.002856\n",
       "2          2  0.999346  0.000363  0.000054  0.000057  0.000180\n",
       "3          3  0.000260  0.002859  0.996193  0.000219  0.000469\n",
       "4          4  0.995207  0.001464  0.001163  0.001277  0.000889\n",
       "...      ...       ...       ...       ...       ...       ...\n",
       "19612  19612  0.000182  0.999574  0.000105  0.000084  0.000055\n",
       "19613  19613  0.000662  0.000085  0.000369  0.000090  0.998794\n",
       "19614  19614  0.000124  0.999625  0.000082  0.000117  0.000052\n",
       "19615  19615  0.000133  0.999480  0.000206  0.000125  0.000057\n",
       "19616  19616  0.990892  0.000276  0.000492  0.000536  0.007804\n",
       "\n",
       "[19617 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission=pd.read_csv('open/sample_submission.csv', encoding='utf-8')\n",
    "sample_submission[['0', '1', '2', '3', '4']] = result\n",
    "sample_submission.to_csv(\"sub_4_1210.csv\", index=False)\n",
    "sample_submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
